{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uh6UnZsWdh",
        "outputId": "260b4dab-fe2a-4483-8e70-551c7f3f416a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# Cloning some utilities from github\n",
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bNlEnGsYOX5A"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from gym import utils\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# **[Exercice 1]** Understanding Value-Function and Q-function\n",
        "\n",
        "In this exercice, we are going to learn:\n",
        "\n",
        "*   What is a MDP?\n",
        "*   How to evaluate the quality of a policy in a MDP (Value-iteration and Policy-Iteration)\n",
        "*   How to move from V-function to Q-function\n",
        "*   How to move from Q-function to greedy-policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "## **[Step 1]** Dealing with MDP and RL environment\n",
        "\n",
        "Here, we are going to use the cleaning robot MDP from\n",
        "http://www.incompleteideas.net/sutton/book/first/3/node7.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f7DVrtCu1xJ6"
      },
      "outputs": [],
      "source": [
        "# @title **[Skip]** Robot MDP implementation\n",
        "\n",
        "class RobotEnv:\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self._RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        self._Ns = 2\n",
        "        self._Na = 3\n",
        "        self._gamma = gamma\n",
        "        \n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        self._P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self._state_decoder  = {0: \"High\", 1: \"Low\"}\n",
        "        self._action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        self._states = np.arange(self.Ns).tolist()\n",
        "        self._action_sets = [np.arange(self.Na).tolist()]*self.Ns\n",
        "\n",
        "    ### Utils\n",
        "    def render_state(self, state):\n",
        "      return self._state_decoder[state]\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self._action_decoder[action] \n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self._state_decoder[i], self._action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return self._states \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return self._action_sets \n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self._P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self._R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self._Ns\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return self._Na\n",
        "\n",
        "    ### Interact with environment\n",
        "    def reward_func(self, state, action, *_):\n",
        "      return self._R[state, action]\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self._P[s,a,:]\n",
        "        next_s = self._RS.choice(self.states, p = prob)\n",
        "        return next_s\n",
        "\n",
        "    def reset(self, new_initial_state=0):\n",
        "        assert new_initial_state < self.Ns\n",
        "        self.state = new_initial_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self._state_decoder[state],\n",
        "            self._action_decoder[action],\n",
        "            self._state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VYqt4muO1yij"
      },
      "outputs": [],
      "source": [
        "# Create the environment\n",
        "env = RobotEnv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbC3m3wO9Imp"
      },
      "source": [
        "A MDP have is a tuple with ($S$, $A$, $R$, $P$, $\\gamma$)\n",
        "*   $S$ is the state space\n",
        "*   $A$ is the action space\n",
        "*   $R$ is the reward function\n",
        "*   $P$ is the transition kernel. If I am in state $s$, and take the action $a$, what is the probability of moving to state $s'$\n",
        "*   $\\gamma$ is the discount factor, i.e., how far in the future you are looking for rewards (gamma=0 means, you just take immediate reward, gammma=0.9 you look at reward around 10 steps away)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FA5vUBd7X9q",
        "outputId": "13db16c5-1019-4e07-8736-9add23697468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of states:  2 ['High', 'Low']\n",
            "Number of actions:  3 ['WAIT', 'SEARCH', 'RECHARGE']\n",
            "\n",
            "Set of states: [0, 1]\n",
            "Set of available actions per state: [[0, 1, 2], [0, 1, 2]]\n",
            "\n",
            "P has shape:  (2, 3, 2)\n",
            "R has shape:  (2, 3)\n",
            "discount factor:  0.5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display some of the MDP relevant information\n",
        "\n",
        "print(\"Number of states: \", env.Ns, [env.render_state(s) for s in range(env.Ns)])\n",
        "print(\"Number of actions: \", env.Na, [env.render_action(a) for a in range(env.Na)])\n",
        "print(\"\")\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of available actions per state:\", env.actions)\n",
        "print(\"\")\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMTkHNuBtw9K"
      },
      "outputs": [],
      "source": [
        "# [P] We want the agent to ignore the action 'wait'\n",
        "# [P] Transition kernel (P): state, actions, next action states\n",
        "# [P] Reward (R): states, actions\n",
        "# The discount is super low, we're not looking too far into the future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_hP0wjEFT4"
      },
      "source": [
        "A MDP is a mathematical representation of an environment. Here, we are going to interact with this environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6S_ja2FDXHP",
        "outputId": "94c91469-16fc-4e71-905a-2b6efd565828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State 0: battery is High\n",
            "Action 1: robot performs SEARCH\n",
            "Reward at state=0 and action=1 is 1.0\n",
            "Next (stochastic) state is High\n"
          ]
        }
      ],
      "source": [
        "state=0\n",
        "action=1\n",
        "print(f\"State {state}: battery is\", env.render_state(state))\n",
        "print(f\"Action {action}: robot performs\", env.render_action(action))\n",
        "print(f\"Reward at state={state} and action={action} is\", env.reward_func(state,action))\n",
        "\n",
        "next_state = env.sample_transition(state,action)\n",
        "print(\"Next (stochastic) state is\", env.render_state(next_state))  # you can keep running this cell colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhlkJazEuHzg"
      },
      "outputs": [],
      "source": [
        "# [P] Simple transition (we suppose to know everything: p, r, rho)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TPqLe-LP_"
      },
      "source": [
        "Finally, we here define a helper to step in the environment. Let's try to follow a random policy by picking a random action $a$ at everytime step $t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OC-zSnd9ExV",
        "outputId": "99ad337e-55fc-437c-fcea-367271723e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    0    0    0.0 \t --> In High do WAIT arrive at High get 0.0\n",
            "0    1    1    1.0 \t --> In High do SEARCH arrive at Low get 1.0\n",
            "1    1    0    -1.0 \t --> In Low do SEARCH arrive at High get -1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Interact with environment\n",
        "\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(4):\n",
        "    action = np.random.randint(env.Na) # Pick random action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19U9wqePueWn"
      },
      "outputs": [],
      "source": [
        "# [P] Interaction loop in model free RL algorithm\n",
        "#  1. action selection step\n",
        "#  2. feeding the action to the enviroment\n",
        "#  3. the return is the new state and the reward \n",
        "#     (and a dumb signal, namely if we're ending in a state with no-exit \n",
        "#     (so the enviroment has to be reset, retrieved an element from the initial distribution))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z7SImS4Fpta"
      },
      "source": [
        "It is also possible to define a deterministic policy which associate an action $a$ for every state $s$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZGmYI3OCKXv",
        "outputId": "82d7ff08-2fe0-4c65-b6d6-65e15d92888a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "random policy =  [1 1]\n",
            "In state High perform SEARCH\n",
            "In state Low perform SEARCH\n"
          ]
        }
      ],
      "source": [
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEd673XTu7fI"
      },
      "outputs": [],
      "source": [
        "# [P] This is a deterministic policy with actions that are fixed:\n",
        "# we'll always perform action 0 in state 0, action 1 in state 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1883coHXIF"
      },
      "source": [
        "### **[Question 1]** Handcrafting the optimal policy \n",
        "Hand-craft the optimal policy (High=search, Low=recharge), display it, and interact with the environment for 5 steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApI4TrT-HWkC",
        "outputId": "77edbc4a-b66c-45a3-920f-27df78beb6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    1    1.0 \t --> In High do SEARCH arrive at Low get 1.0\n",
            "1    2    0    0.0 \t --> In Low do RECHARGE arrive at High get 0.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "my_policy = [1, 2]\n",
        "# [P] Look for garbage when you are in state 0, recharge when in state 1\n",
        "\n",
        "# Interaction loop\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(5):\n",
        "    action = env.actions[state][my_policy[state]] # [P] (Re-check!) Pick action according to the policy \n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMk_C05rGGVs"
      },
      "source": [
        "From now on, you should have understood how to interact with an environment, and retrieve the MDP information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7d9_NsHQXM"
      },
      "source": [
        "## **[Step 2]** Evaluating a policy\n",
        "In this subsection, we aim at estimating the quality of a predefined policy, i.e, how much reward can I expect if I follow any policy (even if this policy is not optimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "\n",
        "### Useful functions\n",
        "In the following exercice, there is a constant back-and-forth between dense and sparse representation of policy. For instance, taking the action $a=2$ may be encoded by:\n",
        "\n",
        "*   Sparse Represention: a=2\n",
        "*   Dense Represention: a=[0, 0, 1]\n",
        "\n",
        "To help you to move from dense and sparse, policy, we provide you those two functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sekFIFmm6V71"
      },
      "outputs": [],
      "source": [
        "def densify_policy(policy, Na):\n",
        "  \"\"\" Turn a dense policy into a sparse one.\n",
        "  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  \"\"\"\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  \"\"\" Turn a sparse determinist policy into a dense one.\n",
        "  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  \"\"\"\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT3iHZ8JNLO"
      },
      "source": [
        "### **[Question 2]** Policy Evaluation\n",
        "Let's start doing things with our policy! Have a look to slide **PLACEHOLDER**, compute the dynamics and rewards given the policy, and solve the linear system on V to evaluate the policy.\n",
        "\n",
        "First, compute the policy normalized transition/rewards\n",
        "$$P^{\\pi}(s, s') = \\sum_a{\\pi(s|a)P(s,a,s')}$$\n",
        "$$R^{\\pi}(s) = \\sum_a{\\pi(s|a)R(s,a)}$$\n",
        "\n",
        "Then, compute the value function, which verifies the Bellman equation,\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "which can be turned into\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOVL2QrdJMrI",
        "outputId": "ad343323-9a0c-43ad-e220-42b617f5b632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## pi:\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "## Vpi:  [1.6 0. ]\n"
          ]
        }
      ],
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "# Policy to evaluate\n",
        "# State A: Search\n",
        "# State B: Wait\n",
        "sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Compute the dynamics given the policy\n",
        "\n",
        "# Naive implementation\n",
        "Ppi = np.zeros([2, 2])  # Hint: Ppi is of shape [2,2] -> it was normalized by the action\n",
        "for state in range(env.Ns):\n",
        "  for next_state in range(env.Ns):\n",
        "    Ppi[state, next_state] = np.sum(P[state, :, next_state] * dense_policy[state])\n",
        "\n",
        "Rpi = np.zeros([2])\n",
        "for state in range(env.Ns):\n",
        "  Rpi[state] = np.sum(R[state]*dense_policy[state])\n",
        "\n",
        "# Matrix form\n",
        "Ppi = np.sum(P * dense_policy[..., None], axis=1) # need to increase the size of dense_policy\n",
        "Rpi = np.sum(R * dense_policy, axis=1)\n",
        "\n",
        "# Evaluate the policy\n",
        "#Vpi = np.linalg.inv(np.eye(2) - gamma* Ppi) * Rpi <- INEFFICIENT BECAUSE YOU INVERT THE MATRIX\n",
        "Vpi = np.linalg.solve(np.eye(2) - gamma * Ppi, Rpi)\n",
        "print(\"## Vpi: \", Vpi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV10O8bjylvf",
        "outputId": "769dd0b1-6f77-4fd7-fed2-aa8e833e0bc5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[2, 0],\n",
              "       [0, 6]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PAU TEST\n",
        "element_1 = [1, 2]\n",
        "element_2 = [[2,0], [0,3]]\n",
        "np.array(element_1) * np.array(element_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-lOPxkn0AxZ",
        "outputId": "7cc5b8fa-2fbc-4f1c-d486-15fa2f2d3218"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PAU TEST\n",
        "Rpi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZfn_6RFy-dr",
        "outputId": "8cff971c-beba-444c-9ad3-fb7610518863"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.5       , 0.        ],\n",
              "       [0.        , 0.33333333]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PAU TEST\n",
        "np.linalg.inv(element_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnODmP6TR1-_"
      },
      "source": [
        "### **[Question 2]** Implement the recursive implementation of value evaluation\n",
        "\n",
        "You want to evaluate the policy by iterating the fixed point equation on V, starting from a randomly initialized V function.\n",
        "\n",
        "In other words:\n",
        "\n",
        "To compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "you can use the contractive property of Bellman \n",
        "$$V^{\\pi}_{k+1} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}_k$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hHiqwbKSCCc",
        "outputId": "e9ab6d7f-5e62-4a2f-ac86-a9cf64304cb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "[1.59999991 0.        ]\n"
          ]
        }
      ],
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Policy evaluation (recursive)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "sparse_policy = np.array([1, 0])\n",
        "dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "print(dense_policy)\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "\n",
        "# Stopping criterion\n",
        "# Feel free to use the any valid stopping criterion (max_iteration or inf_norm)\n",
        "#epsilon = 1e-3\n",
        "epsilon = 1e-7\n",
        "\n",
        "Ppi = np.sum(P * dense_policy[..., None], axis=1) # need to increase the size of dense_policy\n",
        "Rpi = np.sum(R * dense_policy, axis=1)\n",
        "\n",
        "\n",
        "# Estimate V (please print v at each iteration k)\n",
        "v = np.zeros((P.shape[0],))\n",
        "next_v = None\n",
        "while (next_v is None) or (np.linalg.norm(v-next_v) > epsilon):   ## -- STOPPING CONDITION\n",
        "  if next_v is not None:\n",
        "    v = next_v\n",
        "  next_v = Rpi + gamma * Ppi @ v\n",
        "\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcvnxbuOa1z"
      },
      "source": [
        "### **[Question 3]** Turning V-function into Q-function\n",
        "What is the Q-function for this value function ?\n",
        "\n",
        "$$Q^{\\pi}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s')V^{\\pi}(s')$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rLK8scKO0CV",
        "outputId": "5a1f2c3c-cec5-471d-fa41-7493f035b288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Qpi:\n",
            "[[ 0.79999995  1.59999997  0.29999995]\n",
            " [ 0.         -0.20000005  0.79999995]]\n"
          ]
        }
      ],
      "source": [
        "# Compute the Q values\n",
        "\n",
        "''' TO CHECK!!!\n",
        "Qpi = np.zeros([env.Ns, env.Na])\n",
        "# Hint: look at the previous code ;)\n",
        "for state in range(env.Ns):\n",
        "  for action in range(env.Na):\n",
        "    Qpi[state, action] = R[state, action] + gamma * np.sum(P[state, :, next_state] * Vpi[next_state])\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "'''\n",
        "\n",
        "Qpi = R + gamma * np.sum(P * v[None, None], axis=-1) # [P] we have to add two addictional axes \n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzH417WO7Nu"
      },
      "source": [
        "The Q-function is a useful way to evaluate the policy. Yet, it can also be used to improve the policy! To do so, you can create a new policy by taking the argmax of the Q-function (improvment step). \n",
        "\n",
        "What is the next policy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvglCCIjPUaf",
        "outputId": "948b7ec2-37f4-44c9-8d39-85c7aab92b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Qpi:\n",
            "[[ 0.79999995  1.59999997  0.29999995]\n",
            " [ 0.         -0.20000005  0.79999995]]\n",
            "## new pi:\n",
            "[1 2]\n",
            "In state High perform SEARCH\n",
            "In state Low perform RECHARGE\n"
          ]
        }
      ],
      "source": [
        "# Compute the Q values\n",
        "Qpi = R + gamma * np.sum(P * v[None, None], axis=-1) # we have to add two addictional axes \n",
        "# [P] CHECK!!!!!!!!\n",
        "# It probably is: \n",
        "#Qpi = R + gamma * np.sum(P * Vpi[None, None], axis=-1) # we have to add two addictional axes \n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "new_policy = np.argmax(Qpi, axis=-1)\n",
        "print(\"## new pi:\")\n",
        "print(new_policy)\n",
        "print(env.render_policy(new_policy))\n",
        "\n",
        "# Compute the value of the NEW policy\n",
        "new_dense_policy = densify_policy(new_policy, Na=env.Na)\n",
        "\n",
        "new_Ppi = np.sum(P*new_dense_policy[..., None], axis=1)\n",
        "new_Rpi = np.sum(R*new_dense_policy, axis=1)\n",
        "new_Vpi = np.linalg.solve(np.eye(2) - gamma * new_Ppi, new_Rpi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0Iu7xMP9Wp"
      },
      "source": [
        "## **[Step 3]** Policy Improvement in MDP\n",
        "\n",
        "In slide **PLACEHOLDER**, we introduced two algorithms to obtain the optimal policy from a sub-optimal one (by using different shade of policy improvment shapes). \n",
        "\n",
        "*   **Value iteration**: From an initial policy, compute its value exactly, then perform one step of greedy policy improvement.\n",
        "*   **Policy iteration**: From an initial policy, compute its value approximately, then perform one step of greedy policy improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "cellView": "form",
        "id": "aNY2iNLpVYCG"
      },
      "outputs": [],
      "source": [
        "# @title **[Skip]** New Environment\n",
        "\n",
        "\n",
        "class GridWorldWithPits:\n",
        "    def __init__(self, grid, txt_map, gamma=0.99, proba_succ=0.95, uniform_trans_proba=0.001, normalize_reward=False):\n",
        "        self.desc = np.asarray(txt_map, dtype='c')\n",
        "        self.grid = grid\n",
        "        self.txt_map = txt_map\n",
        "\n",
        "        self.action_names = np.array(['right', 'down', 'left', 'up'])\n",
        "\n",
        "        self.n_rows, self.n_cols = len(self.grid), max(map(len, self.grid))\n",
        "\n",
        "        # Create a map to translate coordinates [r,c] to scalar index\n",
        "        # (i.e., state) and vice-versa\n",
        "        self.normalize_reward = normalize_reward\n",
        "\n",
        "\n",
        "        self.initial_state = None\n",
        "        self.coord2state = np.empty_like(self.grid, dtype=np.int)\n",
        "        self.nb_states = 0\n",
        "        self.state2coord = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(len(self.grid[i])):\n",
        "                if self.grid[i][j] != 'w':\n",
        "                    if self.grid[i][j] == 's':\n",
        "                        self.initial_state = self.nb_states\n",
        "                    self.coord2state[i, j] = self.nb_states\n",
        "                    self.nb_states += 1\n",
        "                    self.state2coord.append([i, j])\n",
        "                else:\n",
        "                    self.coord2state[i, j] = -1\n",
        "\n",
        "        self.P = None\n",
        "        self.R = None\n",
        "        self.proba_succ = proba_succ\n",
        "        self.uniform_trans_proba = uniform_trans_proba\n",
        "\n",
        "        # compute the actions available in each state\n",
        "        self.state_actions = [range(len(self.action_names)) for _ in range(self.nb_states)]#self.compute_available_actions()\n",
        "        self.matrix_representation()\n",
        "        self.lastaction = None\n",
        "        self.current_step = 0\n",
        "  \n",
        "        self._actions = self.state_actions\n",
        "        self._gamma = gamma\n",
        "\n",
        "\n",
        "    def matrix_representation(self):\n",
        "        if self.P is None:\n",
        "            nstates = self.nb_states\n",
        "            nactions = max(map(len, self.state_actions))\n",
        "            self.P = np.inf * np.ones((nstates, nactions, nstates))\n",
        "            self.R = np.inf * np.ones((nstates, nactions))\n",
        "            for s in range(nstates):\n",
        "                r, c = self.state2coord[s]\n",
        "                for a_idx, action in enumerate(range(len(self.action_names))):\n",
        "                    self.P[s, a_idx].fill(0.)\n",
        "                    if self.grid[r][c] == 'g':\n",
        "                        self.P[s, a_idx, self.initial_state] = 1.\n",
        "                        self.R[s, a_idx] = 10.\n",
        "                    else:\n",
        "                        ns_succ, ns_fail = np.inf, np.inf\n",
        "                        if action == 0:\n",
        "                            ns_succ = self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ns_fail = [self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[max(0, r - 1), c]\n",
        "                            ]\n",
        "\n",
        "                        elif action == 1:\n",
        "                            ns_succ = self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ns_fail = [self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ]\n",
        "                        elif action == 2:\n",
        "                            ns_succ = self.coord2state[r, max(0, c - 1)]\n",
        "                            ns_fail = [self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ]\n",
        "                        elif action == 3:\n",
        "                            ns_succ = self.coord2state[max(0, r - 1), c]\n",
        "                            ns_fail = [self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[r, max(0, c - 1)]\n",
        "                            ]\n",
        "\n",
        "                        L = []\n",
        "                        for el in ns_fail:\n",
        "                            x, y = self.state2coord[el]\n",
        "                            if self.grid[x][y] == 'w':\n",
        "                                L.append(s)\n",
        "                            else:\n",
        "                                L.append(el)\n",
        "\n",
        "                        self.P[s, a_idx, ns_succ] = self.proba_succ\n",
        "                        for el in L:\n",
        "                            self.P[s, a_idx, el] += (1. - self.proba_succ)/len(ns_fail)\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] + self.uniform_trans_proba / nstates\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] / np.sum(self.P[s, a_idx])\n",
        "\n",
        "                        assert np.isclose(self.P[s, a_idx].sum(), 1)\n",
        "\n",
        "                        if self.grid[r][c] == 'x':\n",
        "                            self.R[s, a_idx] = -20\n",
        "                        else:\n",
        "                            self.R[s, a_idx] = -2\n",
        "\n",
        "            if self.normalize_reward:\n",
        "                minr = np.min(self.R)\n",
        "                maxr = np.max(self.R[np.isfinite(self.R)])\n",
        "                self.R = (self.R - minr) / (maxr - minr)\n",
        "\n",
        "            self.d0 = np.zeros((nstates,))\n",
        "            self.d0[self.initial_state] = 1.\n",
        "\n",
        "    def compute_available_actions(self):\n",
        "        # define available actions in each state\n",
        "        # actions are indexed by: 0=right, 1=down, 2=left, 3=up\n",
        "        state_actions = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "                if self.grid[i][j] == 'g':\n",
        "                    state_actions.append([0])\n",
        "                elif self.grid[i][j] != 'w':\n",
        "                    actions = [0, 1, 2, 3]\n",
        "                    if i == 0:\n",
        "                        actions.remove(3)\n",
        "                    if j == self.n_cols - 1:\n",
        "                        actions.remove(0)\n",
        "                    if i == self.n_rows - 1:\n",
        "                        actions.remove(1)\n",
        "                    if j == 0:\n",
        "                        actions.remove(2)\n",
        "\n",
        "                    for a in copy.copy(actions):\n",
        "                        r, c = i, j\n",
        "                        if a == 0:\n",
        "                            c = min(self.n_cols - 1, c + 1)\n",
        "                        elif a == 1:\n",
        "                            r = min(self.n_rows - 1, r + 1)\n",
        "                        elif a == 2:\n",
        "                            c = max(0, c - 1)\n",
        "                        else:\n",
        "                            r = max(0, r - 1)\n",
        "                        if self.grid[r][c] == 'w':\n",
        "                            actions.remove(a)\n",
        "\n",
        "                    state_actions.append(actions)\n",
        "        return state_actions\n",
        "\n",
        "    def description(self):\n",
        "        desc = {\n",
        "            'name': type(self).__name__\n",
        "        }\n",
        "        return desc\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return self.R[state, action]\n",
        "\n",
        "    def reset(self, s=None):\n",
        "        self.lastaction = None\n",
        "        if s is None:\n",
        "            self.state = self.initial_state\n",
        "        else:\n",
        "            self.state = s\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action_index = self.state_actions[self.state].index(action)\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "\n",
        "        p = self.P[self.state, action_index]\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "        reward = self.R[self.state, action_index]\n",
        "\n",
        "        self.lastaction = action\n",
        "\n",
        "        r, c = self.state2coord[self.state]\n",
        "        done = self.grid[r][c] == 'g'\n",
        "        self.current_step +=1\n",
        "        self.state = next_state\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render_state(self, state):\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[state]\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if self.grid[r][c] == 'x':\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(out[1 + r][2 * c + 1], 'red', highlight=True)\n",
        "        elif self.grid[r][c] == 'g':  # passenger in taxi\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'green', highlight=True)\n",
        "        else:\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'yellow', highlight=True)\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self.action_names[action] \n",
        "\n",
        "    def render_policy(self, pol):\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[self.state]\n",
        "\n",
        "        for s in range(self.Ns):\n",
        "            r, c = self.state2coord[s]\n",
        "            action = pol[s]\n",
        "            # 'right', 'down', 'left', 'up'\n",
        "            if action == 0:\n",
        "                out[1 + r][2 * c + 1] = '>'\n",
        "            elif action == 1:\n",
        "                out[1 + r][2 * c + 1] = 'v'\n",
        "            elif action == 2:\n",
        "                out[1 + r][2 * c + 1] = '<'\n",
        "            elif action == 3:\n",
        "                out[1 + r][2 * c + 1] = '^'\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = GridWorldWithPits(grid=self.grid, txt_map=self.txt_map,\n",
        "                                    proba_succ=self.proba_succ, uniform_trans_proba=self.uniform_trans_proba)\n",
        "        return new_env\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        try:\n",
        "            p = self.P[s, a]\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return np.zeros([self.n_cols, self.n_rows]) \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return range(4)\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self.P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self.R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self.n_cols * self.n_rows\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbNd2WHi4Kb"
      },
      "source": [
        "The environment works as follow:\n",
        "\n",
        "*   At each timestep, you have a negative reward of -2\n",
        "*   If the agent moves to state labelled with X, it receives a negative reward of -20\n",
        "*   If the agent moves to the state labelled with G (goal), it receives a reward of 10, and the trajectory ends\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbuuOZj7VOgs",
        "outputId": "a333538b-259d-4317-970b-8949c59b6cc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ],
      "source": [
        "#@title New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv2pOuPuFajI",
        "outputId": "0d3b8a5f-44c8-42f4-ed9d-1527e4b0f53d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Set of states: [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Set of actions: range(0, 4)\n",
            "Number of states:  12\n",
            "Number of actions:  4\n",
            "P has shape:  (12, 4, 12)\n",
            "R has shape:  (12, 4)\n",
            "discount factor:  0.99\n",
            "\n",
            "initial state:  8\n",
            "reward at (s=0, a=1,s'=1):  -2.0\n",
            "\n",
            "random policy =  [3 3 3 3 1 1 3 3 3 2 3 1]\n",
            "(s, a, s', r):\n",
            "8 3 4 -2.0\n",
            "4 1 8 -2.0\n",
            "8 3 4 -2.0\n",
            "4 1 8 -2.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Relevant information about the environment\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKhH4UyoFVp1",
        "outputId": "b9508963-8aeb-4b72-dabc-a4f853c987f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|<:>:>:>|\n",
            "|>:<:^:<|\n",
            "|^:<:v:^|\n",
            "+-------+\n",
            "\n",
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "|\u001b[43m_\u001b[0m:x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :\u001b[41mx\u001b[0m: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "2 : left\n",
            "+-------+\n",
            "| : : :G|\n",
            "|\u001b[43m_\u001b[0m:x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Definining and running a random policy\n",
        "# Define a fixed random policy\n",
        "sparse_policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(3):\n",
        "    action = sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "cellView": "form",
        "id": "HwIpq7j5SqwU"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "# useful function\n",
        "def plot_v(all_v, v_star):\n",
        "  \n",
        "  all_v = np.array(all_v)\n",
        "  v_star = np.array(v_star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(all_v - v_star).max(axis=1)\n",
        "\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||V* - V||_inf\")\n",
        "\n",
        "# useful function\n",
        "def plot_infnorm(lst, star, name=\"V\"):\n",
        "  \n",
        "  lst = np.array(lst)\n",
        "  star = np.array(star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(lst - star).max(axis=1)\n",
        "  plt.figure()\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||{} - {}*||_inf\".format(name, name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6VKYj2xUWx"
      },
      "source": [
        "### **[Question 4]** Implement Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "LL_Lr6oOGZ3W"
      },
      "outputs": [],
      "source": [
        "def densify_policy(policy, Na):\n",
        "  ### Turn a sparse policy into a dense one.\n",
        "  #  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  ###\n",
        "\n",
        "  Ns = len(policy)\n",
        "  dense_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    dense_policy[i,a]=1\n",
        "  return dense_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  ### Turn a dense determinist policy into a sparse one.\n",
        "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  ###\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iXpp55q8QquM",
        "outputId": "2235d4f2-8380-4813-e3bf-d7775dde6c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|^:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|^:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  Ppi = np.sum(P * dense_policy[..., None], axis=1) # bisogna aumentare la dimensione di dense_policy\n",
        "  Rpi = np.sum(R * dense_policy, axis=1)\n",
        "  Vpi = np.linalg.solve(np.eye(env.Ns) - gamma * Ppi, Rpi)\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi = R + gamma * np.sum(P * Vpi[None, None], axis=-1) # we have to add two addictional axes \n",
        "  new_sparse_policy = np.argmax(Qpi, axis=-1)\n",
        "\n",
        "  # store v\n",
        "  v_all.append(Vpi)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if all(sparse_policy == new_sparse_policy):\n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfgZcQneIUcH",
        "outputId": "735f5c9d-12bc-4b41-a65c-ef2d95729742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: : :\u001b[43m_\u001b[0m|\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(10):\n",
        "    action = new_sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMUC1QlHStsu"
      },
      "source": [
        "### **[Question 5]** Implement Policy Iteration (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rSKtZCytSstg",
        "outputId": "ee21ed4b-08f8-4f6c-efc5-c3e9e8c07f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:^:>:>|\n",
            "|v:>:>:^|\n",
            "|>:v:>:>|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:^:>:>|\n",
            "|v:>:>:^|\n",
            "|>:v:>:>|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# compute optimal policy\n",
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  Ppi = ...\n",
        "  Rpi = ...\n",
        "\n",
        "  v = ...\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi = ...\n",
        "  new_sparse_policy = ...\n",
        "\n",
        "  # store v\n",
        "  v_all.append(v)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if all(...): \n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "# **[Exercice 2]** Q learning\n",
        "Q learning is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **off-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is (potentially) not the **learnt** one, and not the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment.\n",
        "- **Iterate**:\n",
        "  -  Pick an action according to an $\\varepsilon$-greedy version of the argmax policy on $Q$, i.e. with probability $\\varepsilon$, pick a random uniform action, with probability $1 - \\varepsilon$, pick the argmax of $Q(s, a)$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Update $Q$ using the quadruplet $(s, a, r, s')$ with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a'} Q(s', a'))$$\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsmojgubLVL9",
        "outputId": "b64d3f51-86ca-49b8-f864-9f8d8eb30060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:^:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 50000  # Increasee this number when it works for full convergence\n",
        "all_rewards = []\n",
        "all_q_function = []\n",
        "\n",
        "# Training values\n",
        "epsilon = 0.05\n",
        "alpha = 0.1\n",
        "\n",
        "q_function = np.zeros((env.Ns, env.Na))\n",
        "\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action (epsilon greedy)\n",
        "    if np.random.uniform() < epsilon:\n",
        "      action = np.random.randint(env.Na)\n",
        "    else:\n",
        "      action = np.argmax(q_function[state], axis=-1)\n",
        "\n",
        "    # Sample the environment\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    if done:\n",
        "      max_next_q = 0\n",
        "    else:\n",
        "      max_next_q = reward + gamma * np.max(q_function[next_state], axis=-1)  # We do not bootstrap at the end of the environment\n",
        "        \n",
        "    q_function[state, action] = alpha * max_next_q + (1-alpha) * q_function[state, action]\n",
        "\n",
        "    # Store information\n",
        "    all_rewards.append(reward)\n",
        "    all_q_function.append(np.copy(q_function))\n",
        "    \n",
        "    state = next_state\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n",
        "\n",
        "sparse_policy = q_function.argmax(axis=1)\n",
        "print(env.render_policy(sparse_policy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "IZXjcsJMaoqK",
        "outputId": "31617fec-8675-4fdb-8461-d0a78e19d25f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'rewards')"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgVxdXG3zMby7CvyjqsIgICjiAKCoKKYMQlxi2fexCN0awGY9TEFWMSozGJEmPUxMQNiQsoAoq4sCMgKgjCKKCyyL4NzMz5/ujumb59e7vb9J2Z9/c888zt6uququ7qOlV1Tp0SVQUhhBDiR07UGSCEEJL9UFgQQggJhMKCEEJIIBQWhBBCAqGwIIQQEkhe1BnIBK1atdKioqKos0EIITWGJUuWbFPV1l7na6WwKCoqwuLFi6POBiGE1BhE5Au/85yGIoQQEgiFBSGEkEAoLAghhARCYUEIISQQCgtCCCGBRCosRGS0iKwWkbUiMtHlfD0Rec48v0BEiqo/l4QQQiITFiKSC+AvAM4E0BvAxSLS2xHtagA7VLU7gAcB3F+9uSSEEAJEO7IYBGCtqq5T1UMAngUwzhFnHICnzN8vAhgpIlKNecwIG7bvxzufbU3LvRas+xZFE6dhwJ1v4lBZhWucV5Z/hRv+sxTLNux0PX/v9E9x9ZOLMGXJRiTrsr5o4jQUTZyGsvKqPPS/801cNHlewvdSVRRNnIbz//ZBqPjpfJ5BfLu3FG+s/DomTFXx/OINKC0rd71mzeY9WLDuW3y0cRdWbHR/B25Yz6H47pkp5TkZDpVV4PnFG5KuD3ZeXLIRW/YcDIy3a/9h/GfBl0ml+cHn2/D51r2u59Zs3oM7Xl6JnfsPxYSrKq5+chG+2RWcNzdeWLwBRROnpfSMVBX9fjMDcx31V1VxxT8Xuj43VcU1Ty3C17sOJJ1uMkQpLNoD2GA73miGucZR1TIAuwC0dLuZiIwXkcUisnjr1uppOJLl1D/MweVPLEzLvS6cPB8AsGP/Yby6/Ku482u37MGN//0Qr634Guf85X3Xe0yeuw6zV23Bz15Yjnmff5twHr74dl/l7wF3GQ3buq17sXP/Ycxftx0HD7s3ol48PHstAGDJFztCXTvyD++k7XkGcdWTizDh30uxa//hyrA3Vn6Dm19cgYdmrXG95rQH5+LCyfPxnUfew9mPuL8DN1Zs3AUA2Lb3UEDM9POXt9fi5hdX4BWXOpUIm3cfxM9fWI7fvvpJYNyHZq/Br6Z+hKVf7kg4nUv+vgAj//CO67nTHpyLp+Z9geufWRoT/tyiDZi9agtOuG92wukBwC9eXAEAeG3F1wExvZm37lvsPliGyxz191/zv8Cc1Vsx6J74vL24ZCNmfboFQ+57K+l0k6HWKLhVdbKqFqtqcevWnivWs4LD5ZnZcGr7vvhGZW9pYg313tKyhNO1j2j2HDSuP2Br5A+Xu494vPhmd1VvqiJEr+1QgvdPhQ07jN5cWUVVmtYz27y7NK1pWc8SQFp6+Imww+yF73CpU4lgCfswI6o1W/YASLzOhmXjjtie+M4Dhz1iJsaO/ck/o90eedjiU5ei6DwA0QqLTQA62o47mGGucUQkD0BTAIl3fesIbg1reUViDWkys3w5Of7XlCUoHO23K6/Irp0c3RrtXDPDiT7rIOzvs7qfQ16O0TSUpZiuVYScLJg9VsSWJT83Pc1fKu8my6q3L1EKi0UAeohIFxEpAHARgFcccV4BcLn5+7sA3lLuA+uJ24edaEOdDEENQaI9f3sjmeb2N23YhWqlsEjzo7bfrryaq31+rlGmVEfB1rvMBmHhrEsFuenJUyrCIhlh7BR61UVkjgRVtUxEbgAwA0AugCdU9WMRuRPAYlV9BcA/APxLRNYC2A5DoNRI/vjmapSWV+CWM4+uDCuaOA35uYLD5YpBRS3w/IQhlecmz/0cf35rLfYcLMPTVw3CZU8sxOAuLdC+eQO8tHQT1t5zJvIcPaMHZqzGD0d0rzwur9BKnYY9zSDeWPk1Jvx7KabfOAxtm9TDcXfP8o3/67FHxxyf/7cPsOSLqnnnwfdWzbsOKmqB60Z0w4ij2lTmp2fbRvhss6GcHNajFd5ds60y/pVPLsRL159Ume+nrhqEU3q2di3LT59fhpeWbsJRbRtj9eY9+N35/fC94zvCi4OHy9Hrtjdw3fBu+OXoXnHny8or0P3W1wEAN47sgYdnV+kk7H0Wq4f66vKv8INhXdCvQzOMe+Q9LN+4CyWTxsbd157vX5xxVMw7cysXAHy98yCKWhWiaOI0iADr7xuL9dv2YcK/luDZ8SfgrVVbMHfNVjx00QAAwMpNu3DWn9+Lucek8/pi4ksfVR675c3iPwu/BABM/XAjrhveDUUTp+H4ouZ4YcKJMXlcdddo1M/Pxb7SMnz30Xn44YhuuOE/H1bef+0W472u37YP973+Kb47sAMmvvQRHrlkADbvLq3Uoy26dVTle//bnLXYV1qGFxZvwD+vHIQVG3fG6Hqc+bYbFhRNnIap15+IAZ2ax5Vp005jGmr3wcP43qPzcFL3VpXn9hw8jL6/edP3uewrLcMxd8yIC7972qfYuOMAnvygBOcNaI8/XtgfALBtbymKHd+O9b0DwDHtmmDCKd1i8g4YI+ubRvaMC//ucR3w4pKNMfez6jAArLnnzLSNltyIVGehqtNVtaeqdlPVe8yw201BAVU9qKoXqGp3VR2kquuizG8qPPzWWjz2Tnz2rYqzsGR7TPi901dVzllbyq8F67fjpaXGTN2O/cHzrU7rj7BM+LehCPztqx/j3/O/DIx/97RPY47tgsLJwpLtuPG/H8aEWYICQIygAIClX8bOdfspsq1ns3qzMfd985QVPrkGPv5qNwDgb3M+dz1vnxu2CwqgajQBAM0a5sflb7mpnA7igRmrQ8Wb+mHVDK0lpx6d8zlWb96DGR9/g5+9sBwvL6tSRt/jeCcAYgRFEFbds7+bRSXx79USBgtLtuPTr3dXCgqLx99dX/n7sXfW4V/zv8CSL3Zg1ieb8V2btZv9Hcxftx3XP7MUb682DFWc9ctJybb9Mce3BJRz7mdbseqbPZiytKrhddY7N/zq9ZMflAAAXrK9p6fNMDv2kdrHX+1Gj7aN4uJUKNCvQ9O4cKegAIDPzLoOIGmrrrDUGgV3XSM/TUNoP7bsKUW9/PRXEashinpG0UvVUlZegX/P/yKUch0wLHEswgjxZHDLialWcJ33Dpt3Nzbu2B8cySRIJ+GcMrHiP/F+ScwUTJnPfKPzzhc+Ni9m9OWcBgqa8rLyYI+VF6B3c0sniDCxR//p3YTu6cRe1lT1S4FpZfTuJGMEKZWTxW7ZsX7bPtTLy1wViVr75KXMf3reF/j1/1bi3/O93ftnx3IfIw9ugiGVR/sdx/SVH1Yjn+vxPCxFuYVVbddv2xcT7qUbcetQLFhvjMI/MUeGTp2YPck5q7fE39MlnTDvM1GrvlQI2/DbhUW6DSzi0sro3Wsx5RUaY6r62eY9OFRWge37DlVW4qjYsH0/9paWYc0W90VKfjjNb1OxIQ9KJ6zJ4ba9sWaEJdv2JT3FZlFRoXh/bezUw7a9pdi1/zC2mun5lb2iQrF5t/uw3yvcC/tUghcHDpVhn82seeWmXfivqVewN6hTlmzEgUPlCY3a1m7Zi/vfWFXZcw4aHa3cVDXFttscJXr1XXIdJ7wa5Xmfu08DVSjg1Y7v2H8IBw+Xo9SxFmflpt2V5d99MN4U3Fo46lXO3QcPY9veUmzdU4pFJdsr19SEbcAfe+dzHCqrSKkzFHYUY+8oZMok36JW7pRXHdw3/VM8/t56rPjN6Th4qBynPzg35vz0G4ehd7smAIBdNlvqdE29+N1m2O/eTvq+k15fFXPsN0+bCgPvCr8q2akkHP77OWjfrEFK6f/jvfUx+oK1W/Zi1B9jF3V9ud17Oub+N1bh2UUb8NndZ8adsyv0w7zv0x+ci6evGoSTe3qvD/r7u+vxd9v8v115fedrVQvefvbCcvzsheUo7hyv4PXCKvff5nzuq/R2S/vyJxYa13g06N3bNMJ7NqHsNUVU8q37sy6vUIjHzS99fAF6tGmE319wbNy5m55dhocvHuB65U+fXx4XZl8p3c9UdMfkb9JYNG2QHxfuxn2vr8J9r6/CSd1d1w+HJFw78UubXu7bDK+/4MgiSV5f+Q0AY/596974BTR21wP21b5ps5d3uc0Zx7SNXA9QXViWLW60blwPgGFp5IVduQnET4sE8ewiw/lAkFlw2NexYH3yy4fcepTpMlW9aWSPUHXKK73ji1o44iWWfoWq58gCANZs2RtjYGDhtvL8nP7tPO9z4FDwQsB2Ph2UJvXj+93b9iTfeIe1avrYNouRCf2iHY4sksT+ATmH2kCVwmzqhxvxk+eqejJfh7BYeH7RhsA4x94Z2/vp3LIhZny8GV1umR54bW2nSf08bN1TWvlefvTfD11dodj5wdPJ7dn+9LwS3/Nh10dYc/5O/1I3jeyBh2a7uxHxw2ld58bDs9fgxpE9YsL6O+rVQ7PXxKRf5iEc3dpzNxPgx99b7xLTm7mfbcUHAS5o/DpgP7JZ3v1v2VeeU0lBFldBJudut01lbczVTyVeHy94dB6mXHcijktgVJkIHFkkiVUNBO7KPUsBbRcUgHuPx0mQyacb2aBuzRasb9T6HyQoUuF3b/ibvoa1SrLqy5eO6ZhURhxB/HHmZ3FhOwP0FV4uMpxrftLF+H8tCYyTyGA9Uzo4N2uuVCzSkuXaEM8rWTiySBJrhCDibpnkNSzP1ErW7LDOyS6i+Fjj8hDSQMXLmijTSstEqXBpmc/76/sY2iM6f2zZMPXq9p6jyFYmLbY4skgRrwrh1dHKVJvud9//O6FzZhINoE/7JpGka5ENjYhzKqJRPff+2XFFxtSBM8fVaa4Zhr+9E7+AcemXO+MWLVYn1e0KxU6XVoUA3EcWZ/U7srqzk9E6T2GRImXl6vqCvHqU3+w6iLZN6rme27hjP55fHKyvcMNPBt11Tp+Y45JJY0NZvdi5+5w+GH3MEa7nCgtyAQC9j4wVDq/9aFhSaVkkO/dqvY3Zq7Zgwbpo/U46vbYO6NTMPaICsz/dHOfLK9tGFnsPlqGwIBfXDO2CI5vWxyCHAjsZWhQWpHS9l/8zp2l0Jnj758MBxE+FjezVBp1bFmYkzSeuKPY852YqnC4oLFLk4r/Pd50zvcZDYfrkByWerqyH3v82bn4xcX0FkPlpqE4tGmJYz1au5/aZliSffJ3e9SXJmu1awvvDL3fG+caqbpxmzF6N6yWPL8DVTy2O6ywM6+H+zNNFmDUednJEUKHG1GuOSChFehD7D6XWwD2zwN0lzaWPL3ANrw5mr9qSMT1iVFPOFBYpsmnnAV9rjJG92riGX1js7eAuUf73w5MCK+Y402ywa+v43k77Zg1w9JH+U0Z92jdNKc92h2lh+OcVxyedVhR98Q7Nw637uN7mNNDNbr/k21gT3stPLEopX26cYlvPkegCwpwcqTRnTVebVS8vN6XrSxI0e06WUUfHfsv/unqQb/x0PJ+Vvz0DlwzuFHvf1G+bFBQWCfLB2m0Y4thZy0+ROntVvLsBAMjPExzZtH5a8tSysCBQcW4tDGzbOD7NTTsP+A5tAWPuPJEejbPxbOxih+7HCA8hG4YvPBZ4ZZIG+bm4fEiwbshuZu32TOasjt3l0U2hnCpP2ARxol5Kt+45CFVjhOHcTChZvMxxwzKvmqYaZ30a+y0HLdJLx5qqRvXycO+5fWPConL3TmGRIPe9vipurUQyLlkEgr9fVhxXEZIhTC/PaoTsH5Z9la/bWhE73+49lFCPJgt0y2nh3AHOnX69CRKmPxjWJeY4TGPrt+jvxlO7e57zw57LRIVFvfxcVKgmvLjOj8O2RtVrJF5deBkguBHWYWG6obCoIbhtPOK3mtiLHDGmdpxDzGQIaujt2OvZid2q3BF4uVSwp+Gso4t/PcozfjaYrYbB/gzc6NiiYaj7iAR/xLeO7R06XxZ+m1cN83EP4oc9m4l6L5624mtjGiqNkyH2Ox3TrklaBVGinJqAsApqs5NZTBmGqJ4PhUWKHNW2sWulaO7igsBOOpVUuSKeDdWgLrEK1XybS057HtxcJtjJkdj47Zs18G0c7z+/X8xxIsUdcVS4RrBlilY0QHAD/73iDqHuY0zNpJydOPxcdyfSSbBjvcd6eTlJ9VIr0lzWX42p2jxrXAIjuXTw0EX9Y45nfPxN6GuDnl0yncgwUMFdQ3B2mIf3ao2vXCrF8KPa4EGXFbIW9vf95JXuytwZPz658vfdDvPX2HtJjLNCO89fO8Q1HIit7PbpCMscMCau2TpYprDvTzzVt8FwOsWzP7dHLhkQc65k0ljMv2Vk5fF3jzMU6beOid2Bz8nEM+N3t0sUv++uSf08dGjeEK0ahRNKmXAb7zeyCNqDoV5ejq/pcmlZBZ5dZFgSpXMtzhUJKOXvObcPzuxbZZLdtVWha2PYtVVhZVm+f4L7aPztnw8PbaZ9Wu+2KJk0FuP6VwmnRDsfTmHxBxeHhnaOatvY831Y4Q9e6H8PIL7OntA1dfPlMFBYJIhTWHj1KKd+uMl3GGqvaF49BXvw1j3u5rYA0KAgN7AXU2AKA/scuFdb4xbcvGH8h5TIfLfdh1KrRvHrTNymrfYc9Hc9kQ6PuH69NEtfsS2EN8+z+rXLyILLlj6Cyq9OAED9/GArI2snRKcVVhAPv7XW85y1a1wYHpy5Jmb1utf7WGezeDqyqbvlWSKbNrmlsn3/ITQsCG+Z5RzZBdXXMIsHN2wPHo3sdnQMRx3dNvCadEBhkSLlFVrZ8OfnClbdNTpUo2GvZ2Eaba+RAxBOKXfbWfG9dKsnPNwx7ePM/93n9HFdOJXIh2Vv2AZ3ie8J2YWFlX5BwMZLia4RsLjtrCrdgd+r+u0479Gckx+O6OY7j//T06r2VJ79s1Pwn2sG+7okt+jQ3FtnErTvhH3jqqDe56YAZfu7N4/wPZ8s2/aWxvXQgz6fgS77awNV24q++ZOT4845y29P8m+XDgRgWAo2CCFgLZzfxIHD/pYuYSzb7Cv2p1xXNSvwj8urrBXtC++GdG2JK0/qgiU++sN0QWGRIM6FZxWqlSaQV53UJVRvDojtQXldY6/QXvPTYYfODQriBcrLy4z9gp3mmgccm8l832OKIpG506CRlL184hPPTrKWifZpjHc+2+oTMzx5uTm+03J2wdqtdSOc2L0VClLcGjdoYGdvzDr6CJ0wFCZgJZQoTmGR7Pag1mr3nm0bJ3Sdte9Mfp6ktOo6yKjD/j7cXJoDsWW3r33q3LLq/dl3xOvQvAFycwQtXUbr6SYSYSEiLURkpoisMf+7dhVEpFxElpl/r1R3PsOgWrVwyvofpgmwfx/HtPNaEFcVyUtYhG2v3S7/bLP7Tnol2/Zh4a8MHcLy208PlwCqenTzbjk17pzXh2T5j3KbWnCWrWOL2DjJ+sHxctqXKvZGL4wbjLDC9sUJ7nqnIAWrfT+PZJXhlddnUKkqCbZCXlnx38fbOXqpOraeY0UF8NRV/gvtYu8Ri6q6LtRbettp6NyyIZ62nXv1R0Pxpwv7x8W1Rh9HNKmPhrYO3qGyqrp+qFwrR0rVISQsohpZTAQwW1V7AJhtHrtxQFX7m39nV1/23Jnn4lf/cHkFfvuqsVNZIvtVu1XWuDi2YO9PNdxHnEhnTUTQpkl9Y3ewACspOz1NBZ5bw++V/lUndYkLs9xgO5+Lcz43WQdyqTacXthHZFeeVIQ7vuNvKhs2G8Uegieo+PbGJkj5vtsx3/76TcNijhNt0BPBKYgSXcBpcajMR1g4iv+GzerJejaqGjj16UeFAsNcvO+2KCzAO78YEfM+OrcsxDkull/WQr6rh8Z+F3m2UejBQ+U4toPhYyzIijGdRCUsxgF4yvz9FIBzIspHQlz893g/Q9M+qvKPbzVuYXqM9m/XLfpFx3eMEQMndo/3EdS5ZUPce264eXXLfvxntrlz77ylvzF92qPHNuKoeLv2W176CABw9rHeO5ud3LM17j4nuQWN1vuxdtRLF9a0nhtuK9JvGNEj4XUOdpx+o5xmoH07NK38fe3JXX3v5VTi2y2tciS+Qe/ZtpFvvoI8rv5qTK/KPIepb/b0+rRv6hrnbJ+d8PzcmrRpXA9FLRvizgR0VG4ku7bIPj11yeBOaFFYgLGO52d/Qt85tl2lgLOvEj+xW0v8eqy/BWEqRLWfRVtVtVrZbwB4qfPri8hiAGUAJqnq/7xuKCLjAYwHgE6dUl/oFhb7ZjGJmE7aPxDnx2KZ1tm3+mxZWICBnZph6Zc7K8Pe+YW30tFpnteisCC0WWEm+t1eytzmPjqXds0aoGTSWNddyizhY5UpaCcze9ww1yWzx7d9a05F7HNs4yKY+nZoisW3nha366EzrwDw54sHxOz6BsRPQYzr3x5vrdqCl5cZmz3ZDR86tywMfP/v3jyi0vGh3V/TDSO6x9XRq4d2wYndWrnu937GMUdgYKfmnhsNWfkYf7LhL+ygQ0fmVv++069KEHgZdLSxubIZ2r1VzN7fRx/ZBJ9vdbf4ys/NwRzbt9StdaFn3Jh8mhm1djO02u0XJgzBBY/OC7weiH/PXVs3wtLbTvNMCwCOaFq/Unjbleb/+cEJodJMloyNLERkloisdPkbZ4+nxsSzl0jurKrFAC4B8CcR8fRGp6qTVbVYVYtbt45mIxZLVoRpbHfsr+rJecV3hu8tzZz7YTttm6THZ5V/GvUw1GW0lC0kM1WVb5vCcHoP9rxfyGSsfROCOCXJVd2Au0UaYJjJ5jlGQB1bNPTsSefmSELTOWFGFomuYXE+70RGy2F1SQdN6yfr3pYOLcikORlyc2Kfp+Xq3muUlQkyJixUdZSq9nH5exnAZhE5EgDM/67e9lR1k/l/HYA5AAa4xcsWrAoapq7ZrZjC7KonAgzu4u+aIlHW3nMmrhveDSt/e0ZMeLtmmRcW028chsf+77hQcef8fLirOWQi9Ovg/lHZzRPtJKPWKLCZJ+0tLYtpdLzfcXzYA9/tFxcWtlFIxJeVE7teydk45+fmxKysP6ptY089VI4gzknmPT7Tpc5nYD23H4/qYQvzy3k8zum9dKqpGpsjG8vM1bq3JTy37wtel5Mo1rTgEWZHbuTRbTH/lpEpOdxMlKh0Fq8AuNz8fTmAl50RRKS5iNQzf7cCcBKAT6oth0mQSO/F3vPyuixWwS1xvbtUycvNwS9H94ob1leHO4GWjeqFNscsalWYsDmkk26t3efYWxa66y1KkvBc67dI0duIIT48WQWv1/3C0LRBfoyewk2Xcky7KoElIp4ryEUkrmd/6WDvFeJeoy67SXmierQjHMIqkZGJ25ShnSYOb7PWvS3hmcnd6uzPylnGTBOVsJgE4DQRWQNglHkMESkWkcfNOEcDWCwiywG8DUNnkdXCwqrcYZysScyoQXBZgHvrnJzs9uRq7wUmi+XH6qPfuJvr3n++odBe4XJ+ynVD0L1NlUC44LhYn06Z/ICtfT6cwtzevnm1Ve7hmRfWTubfMhIdWzRE3/ZNce6A9jHz/xb2BWOCWCeLw3q0Qg/z+ef6CBI3rG/B6fI7xggk9N0M7jw7diSTiLB55JKB+N358aM7J9Ytrf/WyCIdrsmd2C22oiISYaGq36rqSFXtYU5XbTfDF6vqNebvD1S1r6oea/7/RxR5TQY/t9IWzsrrZonhHFlkGmtqLBmrjnYeLhgS4flrh6Bk0lg0ru9uDnjh8Z1QMmksmricP65zC8z66SmVPS/nlqzOOV8Le0lLJo3FstsN5WJhAqvTrbT2Ora03GM79lwn4/JeE5ky6RpSlxFEA7O8r/5oKB50sf8HYuu1s+3919WDK62vcnIS1/mUTBqL5XfEdgLsK9QTvZ9zJDFjZXgHgS0KC/C948Nv9GV9y9tNi7IMyArbVFf67x06D9ElXbcJU/ftbbaI4aCtfbMG+NWYXnHeZG8cWdWzT8SRmx2rZ+znvM6Lp+eXhIoXdke5VDmha8uYhvQax14SFs6emtUoJbKGw3pu39rmqh+6qD8emLG68jiM/y+LRHrBVsP++GVV7iCKOzfHwxenrt6z7nHfecaIzs31yKAuLSoXl004pRsGdmqG4T3bxJTXqps3jz6qcsdGPyzXNn+b83ll/DF9401x3XZ9dOP4oubYk4RxyOm92+LOccfEdRzuO68veh3RuNIQxJp+3G9adbmtn0gVq1MRpev/qExn6zxh2gP7cFZgzN2/P9FYHW2ZHVr89LSeeNh0XPibs49JKk95Zu/7cBI7l9nNRr0Ia7qbDpo0yMdbLt5znViP2BIs1jNwWww848cnY8P2/XH7q+e56CrG9W+Pm55dFpi+Wz1IZHr+2I7N4p7ri9edGHjdmL5HYPpH/r3ts49tF7PWxe6exGq87F6Ne7ZtjJeuPynuPpbS/frhiW/W9NBFA/DQRfGCzyrzz55fjilLN/re495z++K0B+cmnPZkUwC/tWpLjEuck3u2jjEFt7wSW0K+RWEBfjCsC/7+7vqE0/SCI4s6TJjeo91PTNgGxLlQKxGshVTJKFizTZ8SftbCzLgZ35qtchtZiACzV22OC+8ccoMkN9ynoTI/5Xg4idFjDAlkMaOuQkLcOtUGNmz+7bFO7GZ8h+naOrmR+U2Otblzr244soiIMFYrMSOLEPGX33F6Ql4zndw8uhcmnNINzVzckQeRrPO3TBHW+sWSCVYDbY0svJSUq7+J93TbJeR0iBtu2ayOvW1SVcImkkcPdVFaCJONVMsaVJfcOkojerXBrJ+eEnp9TBANC/Lw4W2nxVliVScUFhERpi3Lixn2BxO0gXwQuTniu6LajyYNsqsqhe2dV8QOLCrfSzuXHmG9vBxXRWsqIwG3TkCYjkGqPdYg89AgEilxJkdKYcxHC/JSS79TwMjRMj137j1it85LB8l+m+mC01AZ5pqhXXDJ4E5xG5S4fUDXDO2CybaFanYFbVRbKQZxUndjoeBNI4N9TlUnYacOrD3Vq8wgBY9+fyCmXF817//wxQPQqNK2/08AABq5SURBVF4eOrd038XNSsvSJ1nM9XHHYuHWaQjqSDz6/YF46fpgvYQf95xrKK1P6dkar94wNOHrE6mPyTpuDLMK3NonxM1z818vHYhLB3dC9zaNK31RWbh1Brz40alVuha3DtnIo9vgvvP64pejU9+5MZvJru5gLaFFYUHlKs5f2zbasfshcvt+7HEB44MUMYa52SkqgH2lhmLbz+NnddK0QT627zsUeprEmkKwewQd3SfW8sau6HUTQlZQM0dD0qllsC7DrdEN6ok785cMuTmSuMGBJDbStUh2ZBHGP5eIdznG9D2y0opqSNdYXV4iC9rs07Jum4CJCC4eVH3+6KKCI4sMEGq5f8gPyHIh0bBe8rqITLJsg+HY0NrLOWr+cMGxOLVXm9Du4ru0KsSQri1xa0hvnVeeVBQXZvWc0zXd4nWX8Sd3xU0jU1/8mCxj+iSnXC1IYPvddFznhtPrrJu34zBEuSguajiyiIiwI3PLrrrQZae7bGLPwepxchjEiF5tEvKXUz8/F/8dH95bZ0eX+esq1/Shb+OL1xTPr8Zkzv10GOxebhMpa36SOoN6+ekTFs6NkY5MwqswEK3patRwZBERYXuhlpVRpjbsSReZcHGQjbgruK3/6XlHWf6qASTmUSDZ59I3jR5Vk1g65MqFCazsrm1QWKSJRK1LwjYI1qjXz0ldNlBXhIUbUk0ji2wikSwm29FxGoWkgnNkkex0ktvOjnWF7G6BajFhGwRLyZftI4t2SQ7raxp+rlDSNbLYV037llQXyS7KS2edd65P8PI/FkQNkOMZg8Iig9x4qrd7g7ANy5TrTsQ/Li8OjhgRn9x5BgZ3aRF6b4qajt8IKl1t2/4QrlNqEoluXGSRzob55B6tMdKmyzrjmORGLdWxuj5bobBIE/YmpIm5NL+1z45zYavcEU3rY2Qah+PppmFBHp67dkjWj3zShZ+DwXRNH2X5jCOA6nHvks6ZzdwcwT+uOL7yONl3VUequSvZbWJTg/izi5dPv4q1/1DtmmqoK/Q6omoTpgcvPBbvrtnmEzs5aoL6J4xp8m1n9Y5z2x4Ga53SoKIWwZGrmbo8sqCwSBMndK3a8tTqtfhVrFaNUnO3QKLBvnvbWf3a4dwBHXxiJ0eUbqiDSGQh39VDk1MGL73ttKSuqw7qsKzgNFRY9hw8HBzJgXNk0di2jWhNsHgh/oR5g7k54rrq14+aMLKoq9Tl75bCIiRu+zU857GYyxISTlv0JbYeUx2uc7WGMA3Hp3eOxvxbRoa6nzXarMurhEn2QmEREre5ac+tMj3s7u2O0Sgraj5h3mFBXk4oh3gA0LyhYc6ZzdNQpO5CYRGS5xZtiAtrELBPs5/Ooi4rymoL6X6FfcwVy2679JH0cO+5fWN2uSPhiURYiMgFIvKxiFSIiOciAhEZLSKrRWStiEyszjza2bhjPxaWbI8L93J5YDkS3Odj8URZUfNJ9/y1dTuOKzLHJYM74emrBkWdjRpJVCOLlQDOA+C5Ma6I5AL4C4AzAfQGcLGI9PaKn0muf2ZpXNjjlxUHNvi3v/yx5zmOLIgTq05UUMOddQzukn1mvNVNJKazqvopENgzGwRgraquM+M+C2AcgE8ynkEHW3aXxoWN6t0Wq77ZnfxNKSuIA8stBnUW2cdz1w6JOguRk806i/YA7IqCjWaYKyIyXkQWi8jirVu3pjUj3zh84VemmUKLz5FF7eXYjs0Sit/D3H7z1KMNdxR9O6TP2yoh6SJjIwsRmQXAbbeUW1X15XSnp6qTAUwGgOLi4ox1zY5p1wQv//AkAFUmskUhdkRzQlFRO1l7z5kJ6zLe+PHJUFXk5eZg9d2jUS8vOze6InWbjAkLVR2V4i02AbA7j+9ghkVKm8b1kGc677GsoQZ5zGe2beK9SpsDi9pJXhKOnQwTbKNCUFCQbCWb3X0sAtBDRLrAEBIXAbgk2izFWqp0aN4Qf710IE7s1tI17pNXeltdcBqq5vLMNYMz4hOKkGwmKtPZc0VkI4AhAKaJyAwzvJ2ITAcAVS0DcAOAGQA+BfC8qnqbF2UAVcXtL690hMXGGdP3yJgN3e00quctiykqai4ndW+FiWf2ijobhFQrUVlDTQUw1SX8KwBjbMfTAUyvxqzFsO9QOZ6e90VM2I0jewReJ2IIFb/BQ132MUMIqXlkszVUVhJm+9ScSncf3gKBsoIQUpOgsPBh5aZdcWFhGnnLXt4vKnUWhJCaBIWFD5td1leEauQlOC5lBSGkJkFh4cOhsniPbmEaees6v93wdu1PfH8MQgiJCgoLHw6Xx6/tS2T6yHIo6MbXuw4klSdCCIkCCgsfdh6Ib+wTmT3yU3Dn5vDRE0JqDmyxfCgsiLcsTsTk1S9qnsfGSYQQko1QWPjgthNeIoppv+0xvXbZI4SQbITCwge3pj6MzqJFobGiu3PLQs84/ehZlBBSg8hm31DR4zIyCDMeWHrbaZ7nSiaNxb7SMhT6uAIhhJBsgyMLH+rnx3sATcdiOgoKQkhNg8LCh1aNXFx7UNVACKmDUFj4oC5aC668JoTURSgsfHAzZmrgMjVFCCG1HQoLDw4eLsdbq7bEhXN9BCGkLkJNqwe3/W8lXliyMS6c+1AQQuoiHFl48PnWvVFngRBCsgYKCw8qvBdfE0JInSOUsBCRQhHJMX/3FJGzRSQ/s1mLFj9XHYQQUtcIO7KYC6C+iLQH8CaA/wPwZKYylQ0s3xi/Sx4hhNRVwgoLUdX9AM4D8FdVvQDAMckmKiIXiMjHIlIhIsU+8UpE5CMRWSYii5NNjxBCSGqEtYYSERkC4FIAV5thqSw4WAlD8DwWIu4IVd2WQlqEEEJSJKyw+DGAWwBMVdWPRaQrgLeTTVRVPwVohkoIITWFUMJCVd8B8I7teB2AGzOVKXvSAN4UEQXwmKpO9oooIuMBjAeATp06VUPWCCGk7uArLETkVbhv6wAAUNWzfa6dBeAIl1O3qurLIfM3VFU3iUgbADNFZJWqzvXIy2QAkwGguLiYpkyEEJJGgkYWvzf/nwej4f+3eXwxgM1+F6rqqNSyBqjqJvP/FhGZCmAQDMssQggh1YivsDCnnyAif1BVu9XSq5m2ThKRQgA5qrrH/H06gDszmaYF11gQQkgsYU1nC02lNgBARLoA8N4zNAAROVdENgIYAmCaiMwww9uJyHQzWlsA74nIcgALAUxT1TeSTTMRnKu3T+7ZujqSJYSQrCURa6g5IrIOxvY/nWEqk5NBVacCmOoS/hWAMebvdQCOTTaNVKhwjCzuPbcPht6ftPEXIYTUeAKFhenmoymAHgB6mcGrVLU0kxmLEqewIISQuk7gNJSqVgC4WVVLVXW5+VdrBQUQv+lRQS79LRJC6jZhW8FZIvJzEekoIi2sv4zmLEKcIwsRocAghNRpwuosLjT//9AWpgC6usSt8ThHFjkCzLvlVOwtLYsmQ4QQEjFhV3B3yXRGsoVv95bie4/NiwkTEbQoLEDLRvUiyhUhhERL6G1VRaQPgN4A6lthqvp0JjIVJVOWbsTnW/fFhHHbbUJIXSeUsBCROwAMhyEspgM4E8B7AGqdsHCDDg8JIXWdsFrb7wIYCeAbVb0SxvqHphnLVYS4Wc1SVhBC6jphhcUB04S2TESaANgCoGPmshUdbntvU1YQQuo6YXUWi0WkGYC/A1gCYC+Aef6X1Ex2HTgcF8ZpKEJIXSesNdT15s9HReQNAE1UdUXmshUdDQviNwCkqCCE1HXCKrj/BcM1+LuquiqzWYqW0rLyuDAOLAghdZ2wOosnABwJ4M8isk5EpojITRnMV2Ss37YvLqxhQWgLY0IIqZWEnYZ6W0TmAjgewAgAEwAcA+ChDOYtEioqos4BIYRkH2GnoWbD2L9iHoB3ARyvqlsymbGoKKfHWUIIiSPsNNQKAIcA9AHQD0AfEWmQsVxFSLmb7SwhhNRxwk5D/QQARKQxgCsA/BPGnty1zllSu2b1gyMRQkgdI+w01A0AhgE4DkAJDIX3u5nLVnT0a98MwJdRZ4MQQrKKsGY+9QH8EcASVa3Vfrrf/OSbqLNACCFZRyidhar+HkA+gP8DABFpLSK10m35rE+r9Pbd2zTCzJ+cHGFuCCEkOwglLEyvs78EcIsZlA/g38kmKiIPiMgqEVkhIlNNVyJu8UaLyGoRWSsiE5NNL1muGdoFPdo2ru5kCSEk6whrDXUugLMB7AMAVf0KQCqt6EwAfVS1H4DPUCWEKhGRXAB/geEOvTeAi0WkdwppJkw+t1IlhBAA4YXFIVVVGFupQkQKU0lUVd+06T7mA+jgEm0QgLWquk5VDwF4FsC4VNJNlPw8CgtCCAFCCAsxXK6+JiKPAWgmIj8AMAuGB9p0cBWA113C2wPYYDveaIZVGx2a18qlJIQQkjCBwsIcUVwA4EUAUwAcBeB2Vf2z33UiMktEVrr8jbPFuRVAGYBnUiqFca/xIrJYRBZv3bo16ftce0rXyt9HUV9BCCEAwpvOLgWwU1V/EfbGqjrK77yIXAHgLAAjTYHkZBNiN1jqYIZ5pTcZwGQAKC4uTnoZdp5tw+1cbr5NCCEAwguLwQAuFZEvYCq5AcBUUCeMiIwGcDOAU1R1v0e0RQB6mCa6mwBcBOCSZNJLBLvYyqFvckIIARBeWJyR5nQfgeEqZKa5C918VZ0gIu0APK6qY1S1zFw5PgNALoAnVPXjNOcjjv8srFq9zZEFIYQYhPUN9UU6E1XV7h7hXwEYYzueDmB6OtMOYud+Y1vV4zo3B2UFIYQYcFcfD6Zcd2LUWSCEkKyBCwkIIYQEQmFBCCEkEAoLQgghgVBYEEIICYTCghBCSCAUFoQQQgKhsCCEEBIIhQUhhJBAKCwIIYQEQmFBCCEkEAoLF4Z0bRl1FgghJKugsLCxYbvhLf2IpvUjzgkhhGQXFBY2zv3r+wCAqR967rFECCF1EgoLG9v3HYo6C4QQkpVQWBBCCAmEwsKGcBtVQghxhcKCEEJIIBQWLpw/sEPUWSCEkKyCwsJGhSoAoF0zms4SQoidSPbgFpEHAHwHwCEAnwO4UlV3usQrAbAHQDmAMlUtzmS+TFlB3QUhhDiIamQxE0AfVe0H4DMAt/jEHaGq/TMtKOxQVBBCSCyRCAtVfVNVy8zD+QCySknAgQUhhMSSDTqLqwC87nFOAbwpIktEZHx1ZSiH0oIQQmLImM5CRGYBOMLl1K2q+rIZ51YAZQCe8bjNUFXdJCJtAMwUkVWqOtcjvfEAxgNAp06dUst7SlcTQkjtI2PCQlVH+Z0XkSsAnAVgpKqlWo67xybz/xYRmQpgEABXYaGqkwFMBoDi4mLX+xFCCEmOSKahRGQ0gJsBnK2q+z3iFIpIY+s3gNMBrKyO/O07VF4dyRBCSI0hKp3FIwAaw5haWiYijwKAiLQTkelmnLYA3hOR5QAWApimqm9UR+Yef3dddSRDCCE1hkjWWahqd4/wrwCMMX+vA3BsdebLoqyCs1iEEGInG6yhCCGEZDkUFoQQQgKhsHChsCA36iwQQkhWQWFhIz/XWGExqnfbiHNCCCHZBYWFjcPlhmK7Sf38iHNCCCHZBYWFjdwcrt0mhBA3KCxslNNklhBCXKGwIIQQEgiFBSGEkEAoLGzUzzcexxFNua0qIYTYobCwcXpvw6P6tSd3jTgnhBCSXVBY2NiwYz+6tS5EXi4fCyGE2GGraKNN43rYsrs06mwQQkjWQWFho0KB9s0bRJ0NQgjJOigsbKgCwv23CSEkDgqLGJT7bxNCiAsUFjaMkUXUuSCEkOyDwsKGgsKCEELcoLCwoaoQTkQRQkgcFBY2OLIghBB3KCxsqILjCkIIcSEyYSEid4nIChFZJiJvikg7j3iXi8ga8+/yTOZJjQQzmQQhhNRIohxZPKCq/VS1P4DXANzujCAiLQDcAWAwgEEA7hCR5pnKkKGzIIQQ4iQyYaGqu22HhTA79g7OADBTVber6g4AMwGMzmS+OLAghJB48qJMXETuAXAZgF0ARrhEaQ9gg+14oxnmdq/xAMYDQKdOnZLKD3UWhBDiTkZHFiIyS0RWuvyNAwBVvVVVOwJ4BsANqaSlqpNVtVhVi1u3bp3cPaB090EIIS5kdGShqqNCRn0GwHQY+gk7mwAMtx13ADAn5Yx5wJEFIYS4E6U1VA/b4TgAq1yizQBwuog0NxXbp5thGYHuPgghxJ0odRaTROQoABUAvgAwAQBEpBjABFW9RlW3i8hdABaZ19ypqtszlSEFV3ATQogbkQkLVT3fI3wxgGtsx08AeKJ68gTOQxFCiAtcwW2DsoIQQtyhsLBDnQUhhLgS6TqLbGNhScbUIYQQUqPhyIIQQkggFBaEEEICobAghBASCIUFIYSQQCgsCCGEBEJhQQghJBAKC0IIIYFQWBBCCAmEwoIQQkggFBaEEEICobAghBASCIUFIYSQQCgsCCGEBEJhQQghJBC6KLfxu/P7oWvrwqizQQghWQeFhY3vHd8x6iwQQkhWwmkoQgghgUQyshCRuwCMA1ABYAuAK1T1K5d45QA+Mg+/VNWzqy+XhBBCLKIaWTygqv1UtT+A1wDc7hHvgKr2N/8oKAghJCIiERaqutt2WAhAo8gHIYSQcESmsxCRe0RkA4BL4T2yqC8ii0VkvoicE3C/8WbcxVu3bk17fgkhpC4jqpnp1IvILABHuJy6VVVftsW7BUB9Vb3D5R7tVXWTiHQF8BaAkar6eVDaxcXFunjx4hRyTwghdQsRWaKqxV7nM6bgVtVRIaM+A2A6gDhhoaqbzP/rRGQOgAEAAoUFIYSQ9BLJNJSI9LAdjgOwyiVOcxGpZ/5uBeAkAJ9UTw4JIYTYydg0lG+iIlMAHAXDdPYLABPM6aZi8/c1InIigMfMODkA/qSq/wh5/63mfZOhFYBtSV5bU2GZ6wYsc90g2TJ3VtXWXicjERbZjIgs9pu3q42wzHUDlrlukKkycwU3IYSQQCgsCCGEBEJhEc/kqDMQASxz3YBlrhtkpMzUWRBCCAmEIwtCCCGBUFgQQggJhMLCRERGi8hqEVkrIhOjzk+iiMgTIrJFRFbawlqIyEwRWWP+b26Gi4g8bJZ1hYgMtF1zuRl/jYhcbgs/TkQ+Mq95WESkeksYj4h0FJG3ReQTEflYRG4yw2ttuUWkvogsFJHlZpl/a4Z3EZEFZj6fE5ECM7yeebzWPF9ku9ctZvhqETnDFp6V34KI5IrIhyLymnlcq8ssIiVm3VsmIovNsOjqtqrW+T8AuTDciHQFUABgOYDeUecrwTKcDGAggJW2sN8BmGj+ngjgfvP3GACvAxAAJwBYYIa3ALDO/N/c/N3cPLfQjCvmtWdmQZmPBDDQ/N0YwGcAetfmcpv5aGT+zgewwMzf8wAuMsMfBXCd+ft6AI+avy8C8Jz5u7dZz+sB6GLW/9xs/hYA/BTAfwC8Zh7X6jIDKAHQyhEWWd3myMJgEIC1qrpOVQ8BeBaGG5Iag6rOBbDdETwOwFPm76cAnGMLf1oN5gNoJiJHAjgDwExV3a6qOwDMBDDaPNdEVeerUcuett0rMlT1a1Vdav7eA+BTAO1Ri8tt5n2veZhv/imAUwG8aIY7y2w9ixcBjDR7kOMAPKuqpaq6HsBaGN9BVn4LItIBwFgAj5vHglpeZg8iq9sUFgbtAWywHW80w2o6bVX1a/P3NwDamr+9yusXvtElPGswpxoGwOhp1+pym9Mxy2DsMjkTRq94p6qWmVHs+awsm3l+F4CWSPxZRM2fANwMw/0PYJShtpdZAbwpIktEZLwZFlndjmRbVVL9qKqKSK20kxaRRgCmAPixqu62T73WxnKrajmA/iLSDMBUAL0izlJGEZGzAGxR1SUiMjzq/FQjQ9XwmdcGwEwRiXG4Wt11myMLg00AOtqOO5hhNZ3N5nAT5v8tZrhXef3CO7iER46I5MMQFM+o6ktmcK0vNwCo6k4AbwMYAmPawer82fNZWTbzfFMA3yLxZxElJwE4W0RKYEwRnQrgIdTuMkOrtmjYAqNTMAhR1u2olTjZ8AdjhLUOhtLLUnAdE3W+kihHEWIV3A8gVhn2O/P3WMQqwxZqlTJsPQxFWHPzdwt1V4aNyYLyCoy51j85wmttuQG0BtDM/N0AwLsAzgLwAmKVvdebv3+IWGXv8+bvYxCr7F0HQ9Gb1d8CgOGoUnDX2jLD2G66se33BwBGR1m3I3/52fIHw5rgMxjzv7dGnZ8k8v9fAF8DOAxj/vFqGPO0swGsATDLVkkEwF/Msn4EoNh2n6tgKP7WArjSFl4MYKV5zSMwV/9HXOahMOZ1VwBYZv6Nqc3lBtAPwIdmmVcCuN0M72p+/GvNRrSeGV7fPF5rnu9qu9etZrlWw2YJk83fAmKFRa0ts1m25ebfx1aeoqzbdPdBCCEkEOosCCGEBEJhQQghJBAKC0IIIYFQWBBCCAmEwoIQQkggFBaE+CAie83/RSJySZrv/SvH8QfpvD8h6YTCgpBwFAFISFjYVhd7ESMsVPXEBPNESLVBYUFIOCYBGGbuLfAT05nfAyKyyNw/4FoAEJHhIvKuiLwC4BMz7H+mM7iPLYdwIjIJQAPzfs+YYdYoRsx7rzT3G7jQdu85IvKiiKwSkWcC9yAgJE3QkSAh4ZgI4OeqehYAmI3+LlU9XkTqAXhfRN404w4E0EcNN9gAcJWqbheRBgAWicgUVZ0oIjeoan+XtM4D0B/AsQBamdfMNc8NgOG24isA78Pwm/Re+otLSCwcWRCSHKcDuMx0Fb4AhhuGHua5hTZBAQA3ishyAPNhOHXrAX+GAvivqpar6mYA7wA43nbvjapaAcO9SVFaSkNIABxZEJIcAuBHqjojJtBwob3PcTwKwBBV3S8ic2D4LkqWUtvvcvAbJtUERxaEhGMPjK1bLWYAuM50kQ4R6SkihS7XNQWwwxQUvWB4+bQ4bF3v4F0AF5p6kdYwtsxdmJZSEJIk7JUQEo4VAMrN6aQnYeynUARgqalk3gr3bSnfADBBRD6F4el0vu3cZAArRGSpql5qC58KY4+K5TC86t6sqt+YwoaQSKDXWUIIIYFwGooQQkggFBaEEEICobAghBASCIUFIYSQQCgsCCGEBEJhQQghJBAKC0IIIYH8Px/0vFIL+hcrAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Plotting evolution of the reward across time\n",
        "average_rewards = np.convolve(all_rewards, np.ones(150)/150, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('rewards')\n",
        "# Now you have to implement the cumulative rewards :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "HwUdN1Xravru",
        "outputId": "5794e65b-150d-4230-f714-b37128366b44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'q[1, 1]')"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Plotting evolution of the q value across time\n",
        "state=1 #@param\n",
        "action=1 #@param\n",
        "one_q = [q[state, action] for q in all_q_function]\n",
        "average_q_values = np.convolve(one_q, np.ones(100)/100, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_q_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')\n",
        "# Now you have to implement the Q error :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "w_VJo-11ubCN"
      },
      "outputs": [],
      "source": [
        "#@title Running the agent:\n",
        "state = env.reset()\n",
        "env.render_state(state)\n",
        "rewards = []\n",
        "for i in range(30):\n",
        "    action = sparse_policy[state]\n",
        "    print(env.render_action(action))\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    print(env.render_state(state))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuCSKJx_uKHg"
      },
      "source": [
        "As a next step, we are going to encapsulate the Q-learning information into a class. The idea is to structure the algorithm such as an Q-agent is trained. Therefore, the training loop should be unchanged, you only need to code the QAgent class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMBabfSguI7m"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Q-Agent\n",
        "# ---------------------------\n",
        "class QAgent:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, done, greedy=False):\n",
        "      pass\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysm4Cq_pxj5T"
      },
      "outputs": [],
      "source": [
        "# Training:\n",
        "q_agent = QAgent(env, gamma=env.gamma, learning_rate=1., epsilon=0.5, min_epsilon=0.01)\n",
        "\n",
        "\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "# main algorithmic loop\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action\n",
        "    action = q_agent.sample_action(state)\n",
        "    \n",
        "    # Sample the environment\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    qvalues = q_agent.update(state, action, next_state, reward, done)\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(reward)\n",
        "    qvalues.append(qvalues)\n",
        "    \n",
        "    state = observation\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xsIjpIOX5P"
      },
      "source": [
        "# **[Exercice 3]** SARSA (Optional)\n",
        "Sarsa is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **on-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is the **learnt** one, i.e. the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment. Pick an action $a$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a | s) = \\frac{\\exp \\frac{Q(s,a)}{\\tau}}{\\sum\\limits_{a'} \\exp \\frac{Q(s,a)}{\\tau}}.$$\n",
        "\n",
        "- **Iterate**: \n",
        "  - Play action $a$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Pick an action $a'$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a' | s') = \\frac{\\exp \\frac{Q(s',a')}{\\tau}}{\\sum\\limits_{a''} \\exp \\frac{Q(s',a'')}{\\tau}}.$$\n",
        "  - Update $Q$ using the quintuplet $(s, a, r, s', a')$ (thus the name SARSA) with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma Q(s', a'))$$\n",
        "  - $a \\leftarrow a'$.\n",
        "  - (Optional) Lower the temperature $\\tau$.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Plot the expected cumulative reward of the algorithms: $t \\mapsto \\sum_{i=1}^t r_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gySnHtrsOX5Q"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon):\n",
        "      # Start with a random policy\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward):\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be0d4nOBOX5R"
      },
      "outputs": [],
      "source": [
        "\n",
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n",
        "\n",
        "\n",
        "# Learn the optimal policy by interacting with the environment\n",
        "...\n",
        "\n",
        "\n",
        "# Plot the value function error \n",
        "...\n",
        "\n",
        "\n",
        "# Plot the expected return \n",
        "...\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Hi!_Summer_School_2022.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
