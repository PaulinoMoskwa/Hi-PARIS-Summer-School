{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Hi! Summer School 2022",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7uh6UnZsWdh",
        "outputId": "83414ae5-5fb1-43c1-a9f6-220ab07eeedb",
        "cellView": "form"
      },
      "source": [
        "#@title Cloning some utilities from github\n",
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNlEnGsYOX5A",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from gym import utils\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# **[Exercice 1]** Understanding Value-Function and Q-function\n",
        "\n",
        "In this exercice, we are going to learn:\n",
        "\n",
        "*   What is a MDP?\n",
        "*   How to evaluate the quality of a policy in a MDP (Value-iteration and Policy-Iteration)\n",
        "*   How to move from V-function to Q-function\n",
        "*   How to move from Q-function to greedy-policy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "## **[Step 1]** Dealing with MDP and RL environment\n",
        "\n",
        "Here, we are going to use the cleaning robot MDP from\n",
        "http://www.incompleteideas.net/sutton/book/first/3/node7.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DVrtCu1xJ6",
        "cellView": "form"
      },
      "source": [
        "# @title **[Skip]** Robot MDP implementation\n",
        "\n",
        "class RobotEnv:\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self._RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        self._Ns = 2\n",
        "        self._Na = 3\n",
        "        self._gamma = gamma\n",
        "        \n",
        "        # Note we add a recharge option in state A with a negative reward (to have a well defined matrix-transition)\n",
        "        self._P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])\n",
        "\n",
        "        self._state_decoder  = {0: \"High\", 1: \"Low\"}\n",
        "        self._action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        self._states = np.arange(self.Ns).tolist()\n",
        "        self._action_sets = [np.arange(self.Na).tolist()]*self.Ns\n",
        "\n",
        "    ### Utils\n",
        "    def render_state(self, state):\n",
        "      return self._state_decoder[state]\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self._action_decoder[action] \n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self._state_decoder[i], self._action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return self._states \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return self._action_sets \n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self._P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self._R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self._Ns\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return self._Na\n",
        "\n",
        "    ### Interact with environment\n",
        "    def reward_func(self, state, action, *_):\n",
        "      return self._R[state, action]\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self._P[s,a,:]\n",
        "        next_s = self._RS.choice(self.states, p = prob)\n",
        "        return next_s\n",
        "\n",
        "    def reset(self, new_initial_state=0):\n",
        "        assert new_initial_state < self.Ns\n",
        "        self.state = new_initial_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self._state_decoder[state],\n",
        "            self._action_decoder[action],\n",
        "            self._state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqt4muO1yij"
      },
      "source": [
        "# create the environment\n",
        "env = RobotEnv()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbC3m3wO9Imp"
      },
      "source": [
        "A MDP have is a tuple with ($S$, $A$, $R$, $P$, $\\gamma$)\n",
        "*   $S$ is the state space\n",
        "*   $A$ is the action space\n",
        "*   $R$ is the reward function\n",
        "*   $P$ is the transition kernel. If I am in state $s$, and take the action $a$, what is the probability of moving to state $s'$\n",
        "*   $\\gamma$ is the discount factor, i.e., how far in the future you are looking for rewards (gamma=0 means, you just take immediate reward, gammma=0.9 you look at reward around 10 steps away)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FA5vUBd7X9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53acea97-18a1-489a-a577-5950985ddeae"
      },
      "source": [
        "# Display some of the MDP relevant information\n",
        "\n",
        "print(\"Number of states: \", env.Ns, [env.render_state(s) for s in range(env.Ns)])\n",
        "print(\"Number of actions: \", env.Na, [env.render_action(a) for a in range(env.Na)])\n",
        "print(\"\")\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of available actions per state:\", env.actions)\n",
        "print(\"\")\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states:  2 ['High', 'Low']\n",
            "Number of actions:  3 ['WAIT', 'SEARCH', 'RECHARGE']\n",
            "\n",
            "Set of states: [0, 1]\n",
            "Set of available actions per state: [[0, 1, 2], [0, 1, 2]]\n",
            "\n",
            "P has shape:  (2, 3, 2)\n",
            "R has shape:  (2, 3)\n",
            "discount factor:  0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9R_hP0wjEFT4"
      },
      "source": [
        "A MDP is a mathematical representation of an environment. Here, we are going to interact with this environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6S_ja2FDXHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56b1583-c947-44e2-b5d8-a9b200869f35"
      },
      "source": [
        "state=0\n",
        "action=1\n",
        "print(f\"State {state}: battery is\", env.render_state(state))\n",
        "print(f\"Action {action}: robot performs\", env.render_action(action))\n",
        "print(f\"Reward at state={state} and action={action}) is\", env.reward_func(state,action))\n",
        "\n",
        "next_state = env.sample_transition(state,action)\n",
        "print(\"Next (stochastic) state is\", env.render_state(next_state))  # you can keep running this cell colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 0: battery is High\n",
            "Action 1: robot performs SEARCH\n",
            "Reward at state=0 and action=1) is 1.0\n",
            "Next (stochastic) state is High\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TPqLe-LP_"
      },
      "source": [
        "Finally, we here define a helper to step in the environment. Let's try to follow a random policy by picking a random action $a$ at everytime step $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OC-zSnd9ExV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086c90e4-4bbc-44b9-cc74-6be5253504db"
      },
      "source": [
        "# Interact with environment\n",
        "\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(4):\n",
        "    action = np.random.randint(env.Na) # Pick random action\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    0    0    0.0 \t --> In High do WAIT arrive at High get 0.0\n",
            "0    2    0    -0.5 \t --> In High do RECHARGE arrive at High get -0.5\n",
            "0    2    0    -0.5 \t --> In High do RECHARGE arrive at High get -0.5\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z7SImS4Fpta"
      },
      "source": [
        "It is also possible to define a deterministic policy which associate an action $a$ for every state $s$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZGmYI3OCKXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a124c8ea-196d-4ce7-df73-4c951892c306"
      },
      "source": [
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "random policy =  [0 1]\n",
            "In state High perform WAIT\n",
            "In state Low perform SEARCH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1883coHXIF"
      },
      "source": [
        "### **[Question 1]** Handcrafting the optimal policy \n",
        "Hand-craft the optimal policy (High=search, Low=recharge), display it, and interact with the environment for 5 steps\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApI4TrT-HWkC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c582946-712f-4c7e-8e14-a5dd7f8d65f5"
      },
      "source": [
        "my_policy = ...\n",
        "\n",
        "# Interaction loop\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state, \":\", env.render_state(state))\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "# Interacting with the environment by tacking random action\n",
        "print(\"s:   a:   s':   r:\")\n",
        "for time in range(5):\n",
        "    action = ... # Pick action according to the policy\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(f\"{state}    {action}    {next_state}    {reward} \\t --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initial state:  0 : High\n",
            "\n",
            "s:   a:   s':   r:\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "0    1    1    1.0 \t --> In High do SEARCH arrive at Low get 1.0\n",
            "1    2    0    0.0 \t --> In Low do RECHARGE arrive at High get 0.0\n",
            "0    1    0    1.0 \t --> In High do SEARCH arrive at High get 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMk_C05rGGVs"
      },
      "source": [
        "From now on, you should have understood how to interact with an environment, and retrieve the MDP information.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z7d9_NsHQXM"
      },
      "source": [
        "## **[Step 2]** Evaluating a policy\n",
        "In this subsection, we aim at estimating the quality of a predefined policy, i.e, how much reward can I expect if I follow any policy (even if this policy is not optimal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "\n",
        "### Useful functions\n",
        "In the following exercice, there is a constant back-and-forth between dense and sparse representation of policy. For instance, taking the action $a=2$ may be encoded by:\n",
        "\n",
        "*   Sparse Represention: a=2\n",
        "*   Dense Represention: a=[0, 0, 1]\n",
        "\n",
        "To help you to move from dense and sparse, policy, we provide you those two functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sekFIFmm6V71"
      },
      "source": [
        "def densify_policy(policy, Na):\n",
        "  \"\"\" Turn a dense policy into a sparse one.\n",
        "  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  \"\"\"\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  \"\"\" Turn a sparse determinist policy into a dense one.\n",
        "  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  \"\"\"\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDT3iHZ8JNLO"
      },
      "source": [
        "### **[Question 2]** Policy Evaluation\n",
        "Let's start doing things with our policy! Have a look to slide **PLACEHOLDER**, compute the dynamics and rewards given the policy, and solve the linear system on V to evaluate the policy.\n",
        "\n",
        "First, compute the policy normalized transition/rewards\n",
        "$$P^{\\pi}(s, s') = \\sum_a{\\pi(s|a)P(s,a,s')}$$\n",
        "$$R^{\\pi}(s) = \\sum_a{\\pi(s|a)R(s,a)}$$\n",
        "\n",
        "Then, compute the value function, which verifies the Bellman equation,\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "which can be turned into\n",
        "$$V^{\\pi} = (I - \\gamma P^{\\pi})^{-1} R^{\\pi}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOVL2QrdJMrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6e2d19-7ad8-4163-b192-1c1ec720ecb6"
      },
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "# Policy to evaluate\n",
        "# State A: Search\n",
        "# State B: Wait\n",
        "sparse_policy = np.array([1, 0])  # sub-optinal policy... on purpose!\n",
        "dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "print(\"## pi:\")\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Compute the dynamics given the policy\n",
        "\n",
        "# Naive implementation\n",
        "Ppi = np.zeros([2, 2])  # Hint: Ppi is of shape [2,2] -> it was normalized by the action\n",
        "for state in range(env.Ns):\n",
        "  for next_state in range(env.Ns):\n",
        "    Ppi[state, next_state] = ...\n",
        "\n",
        "Rpi = np.zeros([2])\n",
        "for state in range(env.Ns):\n",
        "  Rpi[state] = ...\n",
        "\n",
        "# Matrix form\n",
        "Ppi = ...\n",
        "Rpi = ...\n",
        "\n",
        "\n",
        "# Evaluate the policy\n",
        "Vpi = ...\n",
        "print(\"## Vpi: \", Vpi)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## pi:\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "## Vpi:  [1.6 0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnODmP6TR1-_"
      },
      "source": [
        "### **[Question 2]** Implement the recursive implementation of value evaluation\n",
        "\n",
        "You want to evaluate the policy by iterating the fixed point equation on V, starting from a randomly initialized V function.\n",
        "\n",
        "In other words:\n",
        "\n",
        "To compute the value function\n",
        "$$V^{\\pi} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}$$\n",
        "\n",
        "you can use the contractive property of Bellman \n",
        "$$V^{\\pi}_{k+1} = R^{\\pi} + \\gamma P^{\\pi}V^{\\pi}_k$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hHiqwbKSCCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53fe2486-a924-4828-b700-27f8814d940a"
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Policy evaluation (recursive)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "sparse_policy = np.array([1, 0])\n",
        "dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "print(dense_policy)\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "\n",
        "# Stopping criterion\n",
        "# Feel free to use the any valid stopping criterion (max_iteration or inf_norm)\n",
        "epsilon = 1e-3\n",
        "\n",
        "Ppi = ...\n",
        "Rpi = ...\n",
        "\n",
        "\n",
        "# Estimate V (please print v at each iteration k)\n",
        "v = np.zeros((P.shape[0],))\n",
        "next_v = None\n",
        "while (next_v is None) or (...):\n",
        "  if next_v is not None:\n",
        "    v = next_v\n",
        "  next_v = ...\n",
        "\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "In state High perform SEARCH\n",
            "In state Low perform WAIT\n",
            "[1.59937429 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcvnxbuOa1z"
      },
      "source": [
        "### **[Question 3]** Turning V-function into Q-function\n",
        "What is the Q-function for this value function ?\n",
        "\n",
        "$$Q^{\\pi}(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s')V^{\\pi}(s')$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rLK8scKO0CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8694a98e-2f8f-48c8-9b49-9396dea50d4c"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi = np.zeros([env.Ns, env.Na])\n",
        "# Hint: look at the previous code ;)\n",
        "for state in range(env.Ns):\n",
        "  for action in range(env.Na):\n",
        "    Qpi[state, action] = ...\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "\n",
        "Qpi = ..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Qpi:\n",
            "[[ 0.8  1.6  0.3]\n",
            " [ 0.  -0.2  0.8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAzH417WO7Nu"
      },
      "source": [
        "The Q-function is a useful way to evaluate the policy. Yet, it can also be used to improve the policy! To do so, you can create a new policy by taking the argmax of the Q-function (improvment step). \n",
        "\n",
        "What is the next policy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvglCCIjPUaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b554e9a4-400c-4b54-d174-d8e40f02bf67"
      },
      "source": [
        "# Compute the Q values\n",
        "Qpi = ...\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "new_policy = ...\n",
        "print(\"## new pi:\")\n",
        "print(new_policy)\n",
        "print(env.render_policy(new_policy))\n",
        "\n",
        "# Compute the value of the NEW policy\n",
        "new_dense_policy = densify_policy(new_policy, Na=env.Na)\n",
        "\n",
        "new_Ppi = ...\n",
        "new_Rpi = ...\n",
        "new_Vpi = ..."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Qpi:\n",
            "[[ 0.8  1.6  0.3]\n",
            " [ 0.  -0.2  0.8]]\n",
            "## new pi:\n",
            "[1 2]\n",
            "In state High perform SEARCH\n",
            "In state Low perform RECHARGE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu0Iu7xMP9Wp"
      },
      "source": [
        "## **[Step 3]** Policy Improvement in MDP\n",
        "\n",
        "In slide **PLACEHOLDER**, we introduced two algorithms to obtain the optimal policy from a sub-optimal one (by using different shade of policy improvment shapes). \n",
        "\n",
        "*   **Value iteration**: From an initial policy, compute its value exactly, then perform one step of greedy policy improvement.\n",
        "*   **Policy iteration**: From an initial policy, compute its value approximately, then perform one step of greedy policy improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNY2iNLpVYCG",
        "cellView": "form"
      },
      "source": [
        "# @title **[Skip]** New Environment\n",
        "\n",
        "\n",
        "class GridWorldWithPits:\n",
        "    def __init__(self, grid, txt_map, gamma=0.99, proba_succ=0.95, uniform_trans_proba=0.001, normalize_reward=False):\n",
        "        self.desc = np.asarray(txt_map, dtype='c')\n",
        "        self.grid = grid\n",
        "        self.txt_map = txt_map\n",
        "\n",
        "        self.action_names = np.array(['right', 'down', 'left', 'up'])\n",
        "\n",
        "        self.n_rows, self.n_cols = len(self.grid), max(map(len, self.grid))\n",
        "\n",
        "        # Create a map to translate coordinates [r,c] to scalar index\n",
        "        # (i.e., state) and vice-versa\n",
        "        self.normalize_reward = normalize_reward\n",
        "\n",
        "\n",
        "        self.initial_state = None\n",
        "        self.coord2state = np.empty_like(self.grid, dtype=np.int)\n",
        "        self.nb_states = 0\n",
        "        self.state2coord = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(len(self.grid[i])):\n",
        "                if self.grid[i][j] != 'w':\n",
        "                    if self.grid[i][j] == 's':\n",
        "                        self.initial_state = self.nb_states\n",
        "                    self.coord2state[i, j] = self.nb_states\n",
        "                    self.nb_states += 1\n",
        "                    self.state2coord.append([i, j])\n",
        "                else:\n",
        "                    self.coord2state[i, j] = -1\n",
        "\n",
        "        self.P = None\n",
        "        self.R = None\n",
        "        self.proba_succ = proba_succ\n",
        "        self.uniform_trans_proba = uniform_trans_proba\n",
        "\n",
        "        # compute the actions available in each state\n",
        "        self.state_actions = [range(len(self.action_names)) for _ in range(self.nb_states)]#self.compute_available_actions()\n",
        "        self.matrix_representation()\n",
        "        self.lastaction = None\n",
        "        self.current_step = 0\n",
        "  \n",
        "        self._actions = self.state_actions\n",
        "        self._gamma = gamma\n",
        "\n",
        "\n",
        "    def matrix_representation(self):\n",
        "        if self.P is None:\n",
        "            nstates = self.nb_states\n",
        "            nactions = max(map(len, self.state_actions))\n",
        "            self.P = np.inf * np.ones((nstates, nactions, nstates))\n",
        "            self.R = np.inf * np.ones((nstates, nactions))\n",
        "            for s in range(nstates):\n",
        "                r, c = self.state2coord[s]\n",
        "                for a_idx, action in enumerate(range(len(self.action_names))):\n",
        "                    self.P[s, a_idx].fill(0.)\n",
        "                    if self.grid[r][c] == 'g':\n",
        "                        self.P[s, a_idx, self.initial_state] = 1.\n",
        "                        self.R[s, a_idx] = 10.\n",
        "                    else:\n",
        "                        ns_succ, ns_fail = np.inf, np.inf\n",
        "                        if action == 0:\n",
        "                            ns_succ = self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ns_fail = [self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[max(0, r - 1), c]\n",
        "                            ]\n",
        "\n",
        "                        elif action == 1:\n",
        "                            ns_succ = self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ns_fail = [self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[r, max(0, c - 1)],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)]\n",
        "                            ]\n",
        "                        elif action == 2:\n",
        "                            ns_succ = self.coord2state[r, max(0, c - 1)]\n",
        "                            ns_fail = [self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[max(0, r - 1), c],\n",
        "                            self.coord2state[min(self.n_rows - 1, r + 1), c]\n",
        "                            ]\n",
        "                        elif action == 3:\n",
        "                            ns_succ = self.coord2state[max(0, r - 1), c]\n",
        "                            ns_fail = [self.coord2state[min(self.n_rows - 1, r + 1), c],\n",
        "                            self.coord2state[r, min(self.n_cols - 1, c + 1)],\n",
        "                            self.coord2state[r, max(0, c - 1)]\n",
        "                            ]\n",
        "\n",
        "                        L = []\n",
        "                        for el in ns_fail:\n",
        "                            x, y = self.state2coord[el]\n",
        "                            if self.grid[x][y] == 'w':\n",
        "                                L.append(s)\n",
        "                            else:\n",
        "                                L.append(el)\n",
        "\n",
        "                        self.P[s, a_idx, ns_succ] = self.proba_succ\n",
        "                        for el in L:\n",
        "                            self.P[s, a_idx, el] += (1. - self.proba_succ)/len(ns_fail)\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] + self.uniform_trans_proba / nstates\n",
        "                        # self.P[s, a_idx] = self.P[s, a_idx] / np.sum(self.P[s, a_idx])\n",
        "\n",
        "                        assert np.isclose(self.P[s, a_idx].sum(), 1)\n",
        "\n",
        "                        if self.grid[r][c] == 'x':\n",
        "                            self.R[s, a_idx] = -20\n",
        "                        else:\n",
        "                            self.R[s, a_idx] = -2\n",
        "\n",
        "            if self.normalize_reward:\n",
        "                minr = np.min(self.R)\n",
        "                maxr = np.max(self.R[np.isfinite(self.R)])\n",
        "                self.R = (self.R - minr) / (maxr - minr)\n",
        "\n",
        "            self.d0 = np.zeros((nstates,))\n",
        "            self.d0[self.initial_state] = 1.\n",
        "\n",
        "    def compute_available_actions(self):\n",
        "        # define available actions in each state\n",
        "        # actions are indexed by: 0=right, 1=down, 2=left, 3=up\n",
        "        state_actions = []\n",
        "        for i in range(self.n_rows):\n",
        "            for j in range(self.n_cols):\n",
        "                if self.grid[i][j] == 'g':\n",
        "                    state_actions.append([0])\n",
        "                elif self.grid[i][j] != 'w':\n",
        "                    actions = [0, 1, 2, 3]\n",
        "                    if i == 0:\n",
        "                        actions.remove(3)\n",
        "                    if j == self.n_cols - 1:\n",
        "                        actions.remove(0)\n",
        "                    if i == self.n_rows - 1:\n",
        "                        actions.remove(1)\n",
        "                    if j == 0:\n",
        "                        actions.remove(2)\n",
        "\n",
        "                    for a in copy.copy(actions):\n",
        "                        r, c = i, j\n",
        "                        if a == 0:\n",
        "                            c = min(self.n_cols - 1, c + 1)\n",
        "                        elif a == 1:\n",
        "                            r = min(self.n_rows - 1, r + 1)\n",
        "                        elif a == 2:\n",
        "                            c = max(0, c - 1)\n",
        "                        else:\n",
        "                            r = max(0, r - 1)\n",
        "                        if self.grid[r][c] == 'w':\n",
        "                            actions.remove(a)\n",
        "\n",
        "                    state_actions.append(actions)\n",
        "        return state_actions\n",
        "\n",
        "    def description(self):\n",
        "        desc = {\n",
        "            'name': type(self).__name__\n",
        "        }\n",
        "        return desc\n",
        "\n",
        "    def reward_func(self, state, action, next_state):\n",
        "        return self.R[state, action]\n",
        "\n",
        "    def reset(self, s=None):\n",
        "        self.lastaction = None\n",
        "        if s is None:\n",
        "            self.state = self.initial_state\n",
        "        else:\n",
        "            self.state = s\n",
        "        self.current_step = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action_index = self.state_actions[self.state].index(action)\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "\n",
        "        p = self.P[self.state, action_index]\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "        reward = self.R[self.state, action_index]\n",
        "\n",
        "        self.lastaction = action\n",
        "\n",
        "        r, c = self.state2coord[self.state]\n",
        "        done = self.grid[r][c] == 'g'\n",
        "        self.current_step +=1\n",
        "        self.state = next_state\n",
        "\n",
        "        return next_state, reward, done, {}\n",
        "\n",
        "    def render_state(self, state):\n",
        "\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[state]\n",
        "\n",
        "        def ul(x):\n",
        "            return \"_\" if x == \" \" else x\n",
        "\n",
        "        if self.grid[r][c] == 'x':\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(out[1 + r][2 * c + 1], 'red', highlight=True)\n",
        "        elif self.grid[r][c] == 'g':  # passenger in taxi\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'green', highlight=True)\n",
        "        else:\n",
        "            out[1 + r][2 * c + 1] = utils.colorize(ul(out[1 + r][2 * c + 1]), 'yellow', highlight=True)\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def render_action(self, action):\n",
        "      return self.action_names[action] \n",
        "\n",
        "    def render_policy(self, pol):\n",
        "        out = self.desc.copy().tolist()\n",
        "        out = [[c.decode('utf-8') for c in line] for line in out]\n",
        "        r, c = self.state2coord[self.state]\n",
        "\n",
        "        for s in range(self.Ns):\n",
        "            r, c = self.state2coord[s]\n",
        "            action = pol[s]\n",
        "            # 'right', 'down', 'left', 'up'\n",
        "            if action == 0:\n",
        "                out[1 + r][2 * c + 1] = '>'\n",
        "            elif action == 1:\n",
        "                out[1 + r][2 * c + 1] = 'v'\n",
        "            elif action == 2:\n",
        "                out[1 + r][2 * c + 1] = '<'\n",
        "            elif action == 3:\n",
        "                out[1 + r][2 * c + 1] = '^'\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "        return \"\\n\".join([\"\".join(row) for row in out]) + \"\\n\"\n",
        "\n",
        "    def copy(self):\n",
        "        new_env = GridWorldWithPits(grid=self.grid, txt_map=self.txt_map,\n",
        "                                    proba_succ=self.proba_succ, uniform_trans_proba=self.uniform_trans_proba)\n",
        "        return new_env\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        try:\n",
        "            p = self.P[s, a]\n",
        "        except:\n",
        "            raise ValueError(\"Action {} cannot be executed in this state {}\".format(action, self.state))\n",
        "        next_state = np.random.choice(self.nb_states, 1, p=p).item()\n",
        "\n",
        "    ### MDP properties\n",
        "    @property\n",
        "    def states(self):\n",
        "      return np.zeros([self.n_cols, self.n_rows]) \n",
        "\n",
        "    @property\n",
        "    def actions(self):\n",
        "      return range(4)\n",
        "\n",
        "    @property\n",
        "    def transition_matrix(self):\n",
        "      return self.P\n",
        "\n",
        "    @property\n",
        "    def reward_matrix(self):\n",
        "      return self.R\n",
        "    \n",
        "    @property\n",
        "    def gamma(self):\n",
        "      return self._gamma\n",
        "\n",
        "    @property\n",
        "    def Ns(self):\n",
        "      return self.n_cols * self.n_rows\n",
        "\n",
        "    @property\n",
        "    def Na(self):\n",
        "      return 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLbNd2WHi4Kb"
      },
      "source": [
        "The environment works as follow:\n",
        "\n",
        "*   At each timestep, you have a negative reward of -2\n",
        "*   If the agent moves to state labelled with X, it receives a negative reward of -20\n",
        "*   If the agent moves to the state labelled with G (goal), it receives a reward of 10, and the trajectory ends\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbuuOZj7VOgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f491b83-1c58-4b59-852e-23dd04138388"
      },
      "source": [
        "#@title New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv2pOuPuFajI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e8cfbc-d886-4ee6-a8fa-4a8312ca1f2a"
      },
      "source": [
        "#@title Relevant information about the environment\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.transition_matrix.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.reward_matrix.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set of states: [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Set of actions: range(0, 4)\n",
            "Number of states:  12\n",
            "Number of actions:  4\n",
            "P has shape:  (12, 4, 12)\n",
            "R has shape:  (12, 4)\n",
            "discount factor:  0.99\n",
            "\n",
            "initial state:  8\n",
            "reward at (s=0, a=1,s'=1):  -2.0\n",
            "\n",
            "random policy =  [2 3 1 0 1 1 0 3 1 3 2 0]\n",
            "(s, a, s', r):\n",
            "8 1 8 -2.0\n",
            "8 1 8 -2.0\n",
            "8 1 8 -2.0\n",
            "8 1 4 -2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKhH4UyoFVp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9dcf25-213d-4505-8097-ded116527be7"
      },
      "source": [
        "#@title Definining and running a random policy\n",
        "# Define a fixed random policy\n",
        "sparse_policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(env.render_policy(sparse_policy))\n",
        "\n",
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(3):\n",
        "    action = sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|<:^:<:>|\n",
            "|>:>:v:v|\n",
            "|^:<:v:v|\n",
            "+-------+\n",
            "\n",
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "|\u001b[43m_\u001b[0m:x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "|\u001b[43m_\u001b[0m:x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :\u001b[41mx\u001b[0m: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwIpq7j5SqwU"
      },
      "source": [
        "#@title Utility functions\n",
        "# useful function\n",
        "def plot_v(all_v, v_star):\n",
        "  \n",
        "  all_v = np.array(all_v)\n",
        "  v_star = np.array(v_star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(all_v - v_star).max(axis=1)\n",
        "\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||V* - V||_inf\")\n",
        "\n",
        "# useful function\n",
        "def plot_infnorm(lst, star, name=\"V\"):\n",
        "  \n",
        "  lst = np.array(lst)\n",
        "  star = np.array(star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(lst - star).max(axis=1)\n",
        "  plt.figure()\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||{} - {}*||_inf\".format(name, name))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E6VKYj2xUWx"
      },
      "source": [
        "### **[Question 4]** Implement Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL_Lr6oOGZ3W"
      },
      "source": [
        "def densify_policy(policy, Na):\n",
        "  ### Turn a sparse policy into a dense one.\n",
        "  #  Ex: [0, 1], Na=2  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  ###\n",
        "\n",
        "  Ns = len(policy)\n",
        "  dense_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    dense_policy[i,a]=1\n",
        "  return dense_policy\n",
        "\n",
        "def sparsify_policy(policy):\n",
        "  ### Turn a dense determinist policy into a sparse one.\n",
        "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  ###\n",
        "  return np.array(policy).argmax(axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpp55q8QquM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69b24907-2bd1-4605-fd37-423f51694f2e"
      },
      "source": [
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  Ppi = ...\n",
        "  Rpi = ...\n",
        "\n",
        "  Vpi = ...\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi = ...\n",
        "  new_sparse_policy = ...\n",
        "\n",
        "  # store v\n",
        "  v_all.append(Vpi)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if all(...): \n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:<:>:>|\n",
            "|^:^:^:^|\n",
            "|v:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:<:^:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|^:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|^:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfgZcQneIUcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735f5c9d-12bc-4b41-a65c-ef2d95729742"
      },
      "source": [
        "# Start a new episode\n",
        "state = env.reset()\n",
        "print(\"start:\")\n",
        "print(env.render_state(state))\n",
        "\n",
        "# Follow the policy\n",
        "for i in range(10):\n",
        "    action = new_sparse_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    print(action, \":\", env.render_action(action))\n",
        "    print(env.render_state(state))\n",
        "    if done:\n",
        "      break\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start:\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S:\u001b[43m_\u001b[0m: : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: :\u001b[43m_\u001b[0m: |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|S: : :\u001b[43m_\u001b[0m|\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: :\u001b[43m_\u001b[0m|\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "3 : up\n",
            "+-------+\n",
            "| : : :\u001b[42mG\u001b[0m|\n",
            "| :x: : |\n",
            "|S: : : |\n",
            "+-------+\n",
            "\n",
            "0 : right\n",
            "+-------+\n",
            "| : : :G|\n",
            "| :x: : |\n",
            "|\u001b[43mS\u001b[0m: : : |\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMUC1QlHStsu"
      },
      "source": [
        "### **[Question 5]** Implement Policy Iteration (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSKtZCytSstg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee21ed4b-08f8-4f6c-efc5-c3e9e8c07f1a"
      },
      "source": [
        "# compute optimal policy\n",
        "# Compute Policy Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.transition_matrix\n",
        "R = env.reward_matrix\n",
        "gamma = env.gamma \n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "\n",
        "sparse_policy = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "sparse_policy_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  print(env.render_policy(sparse_policy))\n",
        "\n",
        "  # Densify policy to perform matrix operation\n",
        "  dense_policy = densify_policy(sparse_policy, Na=env.Na)\n",
        "\n",
        "  # Compute v (and intermediate values)\n",
        "  Ppi = ...\n",
        "  Rpi = ...\n",
        "\n",
        "  v = ...\n",
        "\n",
        "  # Policy improvement step\n",
        "  Qpi = ...\n",
        "  new_sparse_policy = ...\n",
        "\n",
        "  # store v\n",
        "  v_all.append(v)\n",
        "  sparse_policy_all.append(new_sparse_policy)\n",
        "  print(env.render_policy(new_sparse_policy))\n",
        "\n",
        "  # stopping criterion \n",
        "  if all(...): \n",
        "    break\n",
        "\n",
        "  sparse_policy = new_sparse_policy\n",
        "\n",
        "\n",
        "# Display final results\n",
        "print(\"  ###  Final results  ###\")\n",
        "print(env.render_policy(new_sparse_policy))\n",
        "plot_infnorm(sparse_policy_all, new_sparse_policy, name=\"Pi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "|<:<:<:<|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:^:>:>|\n",
            "|v:>:>:^|\n",
            "|>:v:>:>|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:^:>:>|\n",
            "|v:>:>:^|\n",
            "|>:v:>:>|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|<:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|v:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|v:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n",
            "  ###  Final results  ###\n",
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:>:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "# **[Exercice 2]** Q learning\n",
        "Q learning is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **off-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is (potentially) not the **learnt** one, and not the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment.\n",
        "- **Iterate**:\n",
        "  -  Pick an action according to an $\\varepsilon$-greedy version of the argmax policy on $Q$, i.e. with probability $\\varepsilon$, pick a random uniform action, with probability $1 - \\varepsilon$, pick the argmax of $Q(s, a)$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Update $Q$ using the quadruplet $(s, a, r, s')$ with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a'} Q(s', a'))$$\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsmojgubLVL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d415f55f-12bd-4c71-aabf-52db8d250eec"
      },
      "source": [
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 50000  # Increasee this number when it works for full convergence\n",
        "all_rewards = []\n",
        "all_q_function = []\n",
        "\n",
        "# Training values\n",
        "epsilon = 0.05\n",
        "alpha = 0.1\n",
        "\n",
        "q_function = np.zeros((env.Ns, env.Na))\n",
        "\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action (epsilon greedy)\n",
        "    action = ...\n",
        "\n",
        "    # Sample the environment\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    if not done:\n",
        "      max_next_q = ...\n",
        "    else:\n",
        "      max_next_q = ...  # We do not bootstrap at the end of the environment\n",
        "        \n",
        "    q_function[state, action] = ...\n",
        "\n",
        "    # Store information\n",
        "    all_rewards.append(reward)\n",
        "    all_q_function.append(np.copy(q_function))\n",
        "    \n",
        "    state = next_state\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n",
        "\n",
        "sparse_policy = q_function.argmax(axis=1)\n",
        "print(env.render_policy(sparse_policy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|>:>:>:>|\n",
            "|^:>:^:^|\n",
            "|>:>:>:^|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZXjcsJMaoqK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5eabaa0c-004a-4ace-c41f-0ebecd03c819"
      },
      "source": [
        "#@title Plotting evolution of the reward across time\n",
        "average_rewards = np.convolve(all_rewards, np.ones(150)/150, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_rewards)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('rewards')\n",
        "# Now you have to implement the cumulative rewards :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'rewards')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwUdN1Xravru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "5794e65b-150d-4230-f714-b37128366b44",
        "cellView": "form"
      },
      "source": [
        "#@title Plotting evolution of the q value across time\n",
        "state=1 #@param\n",
        "action=1 #@param\n",
        "one_q = [q[state, action] for q in all_q_function]\n",
        "average_q_values = np.convolve(one_q, np.ones(100)/100, mode='valid')  # average rewards over 100 steps\n",
        "plt.figure()\n",
        "plt.plot(average_q_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel(f'q[{state}, {action}]')\n",
        "# Now you have to implement the Q error :)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'q[1, 1]')"
            ]
          },
          "metadata": {},
          "execution_count": 84
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_VJo-11ubCN",
        "cellView": "form"
      },
      "source": [
        "#@title Running the agent:\n",
        "state = env.reset()\n",
        "env.render_state(state)\n",
        "rewards = []\n",
        "for i in range(30):\n",
        "    action = sparse_policy[state]\n",
        "    print(env.render_action(action))\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    if done:\n",
        "      state = env.reset()\n",
        "    print(env.render_state(state))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuCSKJx_uKHg"
      },
      "source": [
        "As a next step, we are going to encapsulate the Q-learning information into a class. The idea is to structure the algorithm such as an Q-agent is trained. Therefore, the training loop should be unchanged, you only need to code the QAgent class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMBabfSguI7m"
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Agent\n",
        "# ---------------------------\n",
        "class QAgent:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, done, greedy=False):\n",
        "      pass\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysm4Cq_pxj5T"
      },
      "source": [
        "# Training:\n",
        "q_agent = QAgent(env, gamma=env.gamma, learning_rate=1., epsilon=0.5, min_epsilon=0.01)\n",
        "\n",
        "\n",
        "qvalues = []\n",
        "rewards = []\n",
        "\n",
        "state = env.reset()\n",
        "t = 0\n",
        "max_steps = 1000\n",
        "\n",
        "# main algorithmic loop\n",
        "while t < max_steps:\n",
        "    \n",
        "    # Sample the action\n",
        "    action = q_agent.sample_action(state)\n",
        "    \n",
        "    # Sample the environment\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    \n",
        "    # Update q-function\n",
        "    qvalues = q_agent.update(state, action, next_state, reward, done)\n",
        "\n",
        "    # Store information \n",
        "    rewards.append(reward)\n",
        "    qvalues.append(qvalues)\n",
        "    \n",
        "    state = observation\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "    \n",
        "    # iterate\n",
        "    t = t + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xsIjpIOX5P"
      },
      "source": [
        "# **[Exercice 3]** SARSA (Optional)\n",
        "Sarsa is a **model-free** algorithm for estimating the optimal Q-function **online**. \n",
        "\n",
        "Being **model-free** means that it doesn't assume knowledge of $P$ and $r$, only that we can interact with the environment.\n",
        "\n",
        "Being **online** means that we update, and hopefully improve our\n",
        "policy with each step that we are making in the environment.\n",
        "\n",
        "It is an **on-policy** algorithm. This means that the samples we use to update our **learnt** policy are collected with an **acting**  policy that is the **learnt** one, i.e. the one associated to the estimated Q-function.\n",
        "\n",
        "Q-learning works as follows:\n",
        "- **Initialization**: Initialize a current estimated Q-function $Q$ to $0$. Receive an initial state $s$ from the environment. Pick an action $a$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a | s) = \\frac{\\exp \\frac{Q(s,a)}{\\tau}}{\\sum\\limits_{a'} \\exp \\frac{Q(s,a)}{\\tau}}.$$\n",
        "\n",
        "- **Iterate**: \n",
        "  - Play action $a$.\n",
        "  - Observe the next state $s'$ and new reward $r$.\n",
        "  - Pick an action $a'$ according to a softmax version of $Q$ with temperature $\\tau$, i.e. sample action $a$ with probability \n",
        "  $$\\pi(a' | s') = \\frac{\\exp \\frac{Q(s',a')}{\\tau}}{\\sum\\limits_{a''} \\exp \\frac{Q(s',a'')}{\\tau}}.$$\n",
        "  - Update $Q$ using the quintuplet $(s, a, r, s', a')$ (thus the name SARSA) with learning rate $\\alpha$\n",
        "  $$Q(s, a) \\leftarrow (1 - \\alpha) Q(s, a) + \\alpha (r + \\gamma Q(s', a'))$$\n",
        "  - $a \\leftarrow a'$.\n",
        "  - (Optional) Lower the temperature $\\tau$.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Plot the expected cumulative reward of the algorithms: $t \\mapsto \\sum_{i=1}^t r_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gySnHtrsOX5Q"
      },
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon):\n",
        "      # Start with a random policy\n",
        "      pass\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      pass\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward):\n",
        "      pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0d4nOBOX5R"
      },
      "source": [
        "\n",
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n",
        "\n",
        "\n",
        "# Learn the optimal policy by interacting with the environment\n",
        "...\n",
        "\n",
        "\n",
        "# Plot the value function error \n",
        "...\n",
        "\n",
        "\n",
        "# Plot the expected return \n",
        "...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}